"""
SimplerEnvè¯„ä¼°å™¨ - æ·»åŠ Affordanceæ”¯æŒ

åœ¨åŸæœ‰è¯„ä¼°å™¨åŸºç¡€ä¸Šæ·»åŠ affordanceåŠŸèƒ½ï¼Œç”¨äºæµ‹è¯•affordanceå¯¹æ€§èƒ½çš„å½±å“
"""
import collections
import os
import sys
import time
from pathlib import Path

import imageio
import numpy as np
import simpler_env
from simpler_env.utils.env.observation_utils import get_image_from_maniskill2_obs_dict
from typing_extensions import override

import wandb

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", "..", "..")))

from src.agent.configuration_pipeline import TrainPipelineConfig
from src.experiments.env_adapters.language_mapper import PersistentLanguageMapper
from src.experiments.envs.base_evaluator import BaseEvaluator
from src.utils.monitor import setup_logger

# å¯¼å…¥affordanceåŠŸèƒ½
from get_pose_corrected_coordinates import add_affordance_to_observation

os.environ["WANDB__SERVICE_WAIT"] = "300"


class SimplerEvaluatorWithAffordance(BaseEvaluator):
    """
    å¸¦Affordanceçš„SimplerEnvè¯„ä¼°å™¨

    åœ¨æ¯æ¬¡è·å–è§‚æµ‹å›¾åƒæ—¶è‡ªåŠ¨æ·»åŠ affordanceä¿¡æ¯ï¼ˆå¤¹çˆªæœå‘ç®­å¤´ï¼‰
    """

    def __init__(self, pipeline_cfg: TrainPipelineConfig):
        super().__init__(pipeline_cfg)

        self.language_logic_chain = pipeline_cfg.eval_cfg.language_logic_chain

        # Affordanceé…ç½®
        self.use_affordance = pipeline_cfg.eval_cfg.use_affordance
        affordance_color_raw = pipeline_cfg.eval_cfg.affordance_color
        # ç¡®ä¿é¢œè‰²æ˜¯ tuple æ ¼å¼ï¼ˆOpenCVéœ€è¦ï¼‰
        self.affordance_color = tuple(affordance_color_raw) if isinstance(affordance_color_raw, list) else affordance_color_raw
        self.affordance_thickness = pipeline_cfg.eval_cfg.affordance_thickness
        self.affordance_length = pipeline_cfg.eval_cfg.affordance_length
        self.affordance_show_point = pipeline_cfg.eval_cfg.affordance_show_point

        # ç»Ÿè®¡ä¿¡æ¯
        self.affordance_stats = {
            'total_frames': 0,
            'affordance_added': 0,
            'affordance_failed': 0
        }

        if self.use_affordance:
            print("=" * 60)
            print("ğŸ¯ AffordanceåŠŸèƒ½å·²å¯ç”¨")
            print(f"  é¢œè‰²: {self.affordance_color}")
            print(f"  ç²—ç»†: {self.affordance_thickness}")
            print(f"  é•¿åº¦: {self.affordance_length}m")
            print(f"  æ˜¾ç¤ºä½ç½®ç‚¹: {self.affordance_show_point}")
            print("=" * 60)

        if self.language_logic_chain:
            language_mapping_candidates = {
                "carrot": ["the yellow vegetable", "the veggie", "the yellow thing that rabbit likes", "the veggie that rabbit likes"],
                "eggplant": ["the purple vegetable", "the veggie", "the thing that looks like a purple balloon"],
                "spoon": ["the silver spoon", "the thing that people use to eat soup", "the shiny thing"],
                "cube": ["the thing that looks like a box", "lego"],
            }
            self.language_mapper = PersistentLanguageMapper(mapping_candidates=language_mapping_candidates, seed=self.seed)

    def add_affordance_to_image(self, img, env, obs):
        """
        ä¸ºå›¾åƒæ·»åŠ affordance

        Args:
            img: åŸå§‹RGBå›¾åƒ
            env: SimplerEnvç¯å¢ƒå®ä¾‹
            obs: ç¯å¢ƒè§‚æµ‹å­—å…¸

        Returns:
            æ·»åŠ äº†affordanceçš„å›¾åƒ
        """
        if not self.use_affordance:
            return img

        self.affordance_stats['total_frames'] += 1

        try:
            # æ·»åŠ affordanceåˆ°è§‚æµ‹
            obs_with_aff = add_affordance_to_observation(
                obs, env,
                arrow_length=self.affordance_length,
                arrow_color=self.affordance_color,
                arrow_thickness=self.affordance_thickness,
                show_point=self.affordance_show_point
            )

            # è·å–æ·»åŠ äº†affordanceçš„å›¾åƒ
            img_with_aff = get_image_from_maniskill2_obs_dict(env, obs_with_aff)

            self.affordance_stats['affordance_added'] += 1
            return np.ascontiguousarray(img_with_aff)

        except Exception as e:
            self.affordance_stats['affordance_failed'] += 1
            if self.affordance_stats['affordance_failed'] <= 3:  # åªæ‰“å°å‰3æ¬¡é”™è¯¯
                print(f"âš ï¸ Affordanceæ·»åŠ å¤±è´¥: {e}")
            return img

    @override
    def evaluate(self):
        '''Run evaluation on all tasks in the task list'''
        # é‡ç½®ç»Ÿè®¡ä¿¡æ¯
        self.affordance_stats = {
            'total_frames': 0,
            'affordance_added': 0,
            'affordance_failed': 0
        }

        # å®ç°è¯„ä¼°é€»è¾‘ï¼ˆå¤åˆ¶è‡ªSimplerEvaluatorï¼‰
        for gradient_step in self.gradient_steps:
            if self.no_gradient_steps:
                model_path = Path(self.eval_cfg.pretrained_model_path)
            else:
                model_path = Path(self.eval_cfg.pretrained_model_path) / f"step_{gradient_step!s}"

            self._initialze_model_client(model_path=str(model_path), gradient_step=gradient_step)

            for task_name in self.task_lists:
                if not self.debug:
                    self._update_n_eval_episode(task_name) # some tasks have different number of possible episodes
                self.evaluate_task(task_name)

            if self.use_wandb:
                wandb.log(self.wandb_metrics, step=int(gradient_step), commit=True)

        # æ‰“å°affordanceç»Ÿè®¡
        if self.use_affordance:
            print("\n" + "=" * 60)
            print("ğŸ“Š Affordanceç»Ÿè®¡ä¿¡æ¯:")
            print(f"  æ€»å¸§æ•°: {self.affordance_stats['total_frames']}")
            print(f"  æˆåŠŸæ·»åŠ : {self.affordance_stats['affordance_added']}")
            print(f"  æ·»åŠ å¤±è´¥: {self.affordance_stats['affordance_failed']}")
            if self.affordance_stats['total_frames'] > 0:
                success_rate = self.affordance_stats['affordance_added'] / self.affordance_stats['total_frames']
                print(f"  æˆåŠŸç‡: {success_rate*100:.1f}%")
            print("=" * 60)

    @override
    def evaluate_task(self, task_name):
        '''
        Evaluates a single task using the trained model.

        Args:
            task_name: str, the name of the task to evaluate
        '''
        # åˆ›å»ºä»»åŠ¡æ—¥å¿—ç›®å½•
        task_log_dir = Path(self.log_dir) / "task_logs"
        if self.main_rank:
            task_log_dir.mkdir(parents=True, exist_ok=True)

        task_logger = setup_logger(
            main_rank=self.main_rank,
            filename=task_log_dir / f"{task_name}.log" if not self.debug else None,
            debug=self.debug,
            name=f'{task_name}_logger'
        )
        task_logger.info(f"Evaluating task: {task_name}")

        env = simpler_env.make(task_name)
        elapsed_steps = 0

        instruction = None
        if self.language_logic_chain:
            mapper = PersistentLanguageMapper()
            for key in self.language_logic_chain.keys():
                if key in task_name:
                    mapper.update(key, self.language_logic_chain[key])
            obs, reset_info = env.reset()
            instruction = reset_info.get("text_plan", ["default instruction"])[0]
            for old, new in mapper.mapping.items():
                instruction = instruction.replace(old, new)
        else:
            obs, reset_info = env.reset(seed=self.seed)
            instruction = reset_info.get("text_plan", ["default instruction"])[0]

        episode_highest_rewards = []

        for i_episode in range(self.n_eval_episode):
            episode_return, episode_highest_reward = 0.0, 0.0
            elapsed_steps = 0

            obs, reset_info = env.reset(seed=self.seed + i_episode)

            recording = i_episode < self.n_video and self.pipeline_cfg.eval_cfg.recording
            if recording:
                current_time = time.strftime("%Y%m%d-%H%M%S")
                video_default_path = Path(self.log_dir) / f"{task_name}_episode_{i_episode}_{current_time}.mp4"
                video_default_path.parent.mkdir(parents=True, exist_ok=True)
                video_writer = imageio.get_writer(video_default_path)

            task_logger.info(
                f"Reset info: {reset_info} Instruction: {instruction} Max episode length: {env.spec.max_episode_steps}"
            )

            # Set up receding horizon control
            action_plan = collections.deque()
            while True:
                # è·å–åŸå§‹å›¾åƒ
                img = np.ascontiguousarray(get_image_from_maniskill2_obs_dict(env, obs))

                # ğŸ¯ æ·»åŠ affordanceï¼ˆå¦‚æœå¯ç”¨ï¼‰
                img_for_policy = self.add_affordance_to_image(img, env, obs)

                if not action_plan:
                    # action horizon is all executed
                    # Query model to get action
                    element = {
                        "observation.images.top": img_for_policy,  # ä½¿ç”¨å¸¦affordanceçš„å›¾åƒ
                        "observation.state": obs,
                        "task": str(instruction)
                    }
                    action_chunk = self.client.infer(element)
                    action_plan.extend(action_chunk[: self.action_step])

                action = action_plan.popleft()
                obs, reward, success, truncated, info = env.step(action.copy())

                # Record video frame if enabled
                # æ³¨æ„ï¼šè§†é¢‘è®°å½•ä½¿ç”¨å¸¦affordanceçš„å›¾åƒ
                if recording:
                    video_writer.append_data(img_for_policy if self.use_affordance else img)

                elapsed_steps += 1
                episode_return += reward
                episode_highest_reward = max(episode_highest_reward, reward)

                if success or truncated or elapsed_steps >= env.spec.max_episode_steps:
                    episode_highest_rewards.append(episode_highest_reward)

                    task_logger.info(
                        f"Episode {i_episode}: success={success}, truncated={truncated}, "
                        f"steps={elapsed_steps}, return={episode_return:.2f}, "
                        f"highest_reward={episode_highest_reward:.2f}"
                    )

                    if recording:
                        video_writer.close()
                        task_logger.info(f"Video saved: {video_default_path}")

                    break

        env.close()

        # Calculate metrics
        success_rate = np.mean(episode_highest_rewards) * 100
        task_logger.info(f"Task {task_name} completed: Success rate = {success_rate:.2f}%")

        return {
            "task_name": task_name,
            "success_rate": success_rate,
            "episode_rewards": episode_highest_rewards,
            "n_episodes": self.n_eval_episode
        }

    def _update_n_eval_episode(self, task_name):
        """æ›´æ–°è¯„ä¼°episodeæ•°é‡ï¼ˆå¤åˆ¶è‡ªSimplerEvaluatorï¼‰"""
        if "google_robot" in task_name:
            if 'coke' in task_name:
                self.n_eval_episode = 25 * 4 # 25 locations, 4 urdfs, 10 trials each
            elif 'move' in task_name:
                self.n_eval_episode = 60 * 4 # 60 locations, 4 urdfs, 10 trials each
            elif 'drawer' in task_name:
                self.n_eval_episode = 3 * 4 * 9 # 3 drawers, 4 urdfs, 9 locations/rgb_overlay_paths, 10 trials each
            elif 'apple' in task_name:
                self.n_eval_episode = 9 * 4 * 3 # 9 apple locations, 4 urdfs, 3 robot locations/rgb_overlay_paths, 10 trials each
            self.n_video = self.n_eval_episode

    def _process_episode_stats(self, metric, episode_stats, success):
        '''
        Process episode stats to extract relevant information
        '''
        # Extract relevant information from episode_stats
        metric['Success Rate'].append(success)
        metric['Move Correct'].append(episode_stats.get('moved_correct_obj', 0))
        metric['Wrong Obj Attempt'].append(episode_stats.get('moved_wrong_obj', 0))
        metric['Grasp Correct'].append(episode_stats.get('is_src_obj_grasped', 0))
        metric['Src Intention Correct'].append(episode_stats.get('source_intention', 0))

    def _aggregate_metrics(self, metrics):
        '''
        Aggregate metrics across all episodes
        '''
        aggregated_metrics = {}
        for key in metrics.keys():
            aggregated_metrics[key] = np.mean(metrics[key])
        return aggregated_metrics

    def _log_summary(self, logger, cnt_episode, eval_time, metrics):
        """è®°å½•è¯„ä¼°æ€»ç»“"""
        logger.info(f"Evaluated {cnt_episode} episodes in {eval_time:.2f} seconds")
        for key, value in metrics.items():
            logger.info(f"{key}: {value:.4f}")
        logger.info("=" * 50)

    def _preprocess_task_instruction(self, instruction):
        """é¢„å¤„ç†ä»»åŠ¡æŒ‡ä»¤"""
        if self.language_logic_chain:
            return self.language_mapper.map(instruction)
        return instruction

