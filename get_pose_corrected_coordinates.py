#!/usr/bin/env python3
"""
åæ ‡ç³»ä¿®æ­£ç‰ˆæœ¬ï¼šæœºæ¢°è‡‚æœ«ç«¯6Dä½å§¿è·å–å’Œå›¾åƒæ ‡æ³¨
ä½¿ç”¨æ­£ç¡®çš„åæ ‡ç³»çº¦å®šå’Œæ›´å‡†ç¡®çš„3Dåˆ°2DæŠ•å½±
"""

import numpy as np
import cv2
import simpler_env
from simpler_env.utils.env.observation_utils import get_image_from_maniskill2_obs_dict
from scipy.spatial.transform import Rotation
import warnings

# æŠ‘åˆ¶è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning, module="gymnasium")

def get_robot_pose_and_image():
    """è·å–æœºæ¢°è‡‚ä½å§¿å’Œç¯å¢ƒå›¾åƒ"""
    print("=== è·å–æœºæ¢°è‡‚ä½å§¿å’Œå›¾åƒ ===\n")

    task_name = "widowx_carrot_on_plate"
    env = simpler_env.make(task_name)

    try:
        obs, reset_info = env.reset(seed=42)

        if "extra" in obs and "tcp_pose" in obs["extra"]:
            tcp_pose = obs["extra"]["tcp_pose"]
            position = tcp_pose[:3]        # ä½ç½® (x, y, z)
            quaternion = tcp_pose[3:7]     # å››å…ƒæ•° (w, x, y, z)

            print("âœ“ æˆåŠŸè·å–å¤¹çˆªæœ«ç«¯ä½å§¿:")
            print(f"  ä½ç½® (m): {position}")
            print(f"  å››å…ƒæ•° (wxyz): {quaternion}")

        else:
            raise KeyError(f"æœªåœ¨obsä¸­æ‰¾åˆ°tcp_pose")

        # è·å–å¤¹çˆªçŠ¶æ€
        gripper_width = 0.5
        try:
            agent = env.unwrapped.agent if hasattr(env, 'unwrapped') else env.agent
            if hasattr(agent, 'gripper_closedness'):
                closedness = agent.gripper_closedness
                gripper_width = max(0.0, min(1.0, 1.0 - closedness))
        except Exception:
            pass

        print(f"  å¤¹çˆªå¼€åˆåº¦: {gripper_width:.3f}")

        # è·å–å›¾åƒ
        print("\nâœ“ è·å–ç¯å¢ƒå›¾åƒ:")
        image = None
        camera_name = "unknown"

        if "image" in obs:
            cam_imgs = obs["image"]
            if "3rd_view_camera" in cam_imgs and "rgb" in cam_imgs["3rd_view_camera"]:
                image = cam_imgs["3rd_view_camera"]["rgb"]
                camera_name = "3rd_view_camera"
            elif "base_camera" in cam_imgs and "rgb" in cam_imgs["base_camera"]:
                image = cam_imgs["base_camera"]["rgb"]
                camera_name = "base_camera"

        if image is None:
            image = get_image_from_maniskill2_obs_dict(env, obs)
            camera_name = "default"

        print(f"  å›¾åƒå°ºå¯¸: {image.shape}")
        print(f"  ä½¿ç”¨ç›¸æœº: {camera_name}")

        # è·å–ç›¸æœºå‚æ•°
        camera_params = obs.get("camera_param", None)

        # è·å–ç‰©ä½“åæ ‡ä¿¡æ¯
        objects_info = get_object_coordinates_from_env(env, obs, reset_info)

        return {
            'position': position,
            'quaternion': quaternion,
            'gripper_width': gripper_width,
            'image': image,
            'camera_params': camera_params,
            'camera_name': camera_name,
            'objects_info': objects_info,
            'env': env,
            'obs': obs
        }

    except Exception as e:
        env.close()
        raise e

def project_3d_to_2d(point_3d, intrinsic_matrix, extrinsic_matrix):
    """å°†3Dç‚¹æŠ•å½±åˆ°2Då›¾åƒåæ ‡"""
    point_3d_homo = np.array([point_3d[0], point_3d[1], point_3d[2], 1.0])
    point_cam = extrinsic_matrix @ point_3d_homo

    if point_cam[2] <= 0:
        return None

    point_2d_homo = intrinsic_matrix @ point_cam[:3]
    u = point_2d_homo[0] / point_2d_homo[2]
    v = point_2d_homo[1] / point_2d_homo[2]

    return [int(u), int(v)]

def draw_coordinate_frame_correct(image, origin_2d, position_3d, quaternion,
                                 intrinsic_matrix, extrinsic_matrix, axis_length=0.05):
    """
    ä½¿ç”¨çœŸå®3Dåˆ°2DæŠ•å½±ç»˜åˆ¶æ­£ç¡®çš„åæ ‡ç³»

    Args:
        origin_2d: åŸç‚¹åœ¨å›¾åƒä¸­çš„2Dåæ ‡
        position_3d: åŸç‚¹çš„3Dä¸–ç•Œåæ ‡
        quaternion: å§¿æ€å››å…ƒæ•° [w, x, y, z]
        axis_length: åæ ‡è½´é•¿åº¦ (ç±³)
    """
    try:
        # å°†å››å…ƒæ•°è½¬æ¢ä¸ºæ—‹è½¬çŸ©é˜µ
        # scipyä½¿ç”¨ [x, y, z, w] æ ¼å¼
        quat_scipy = [quaternion[1], quaternion[2], quaternion[3], quaternion[0]]
        rotation = Rotation.from_quat(quat_scipy)
        rotation_matrix = rotation.as_matrix()

        # å®šä¹‰æ ‡å‡†åæ ‡è½´å‘é‡ (åœ¨æœ«ç«¯æ‰§è¡Œå™¨åæ ‡ç³»ä¸­)
        # ROS/ManiSkillçº¦å®š: Xå‰, Yå·¦, Zä¸Š
        axes_3d = np.array([
            [axis_length, 0, 0],  # Xè½´: å‘å‰
            [0, axis_length, 0],  # Yè½´: å‘å·¦
            [0, 0, axis_length]   # Zè½´: å‘ä¸Š
        ])

        # åº”ç”¨æ—‹è½¬å¾—åˆ°ä¸–ç•Œåæ ‡ç³»ä¸­çš„è½´æ–¹å‘
        rotated_axes = rotation_matrix @ axes_3d.T

        # è®¡ç®—è½´ç«¯ç‚¹çš„3Dä¸–ç•Œåæ ‡
        axis_endpoints_3d = position_3d[:, np.newaxis] + rotated_axes

        # æŠ•å½±è½´ç«¯ç‚¹åˆ°2Då›¾åƒ
        colors = [(0, 0, 255), (0, 255, 0), (255, 0, 0)]  # çº¢ç»¿è“
        labels = ['X(å‰)', 'Y(å·¦)', 'Z(ä¸Š)']

        u_orig, v_orig = origin_2d

        for i, (axis_end_3d, color, label) in enumerate(zip(axis_endpoints_3d.T, colors, labels)):
            # æŠ•å½±è½´ç«¯ç‚¹åˆ°2D
            end_2d = project_3d_to_2d(axis_end_3d, intrinsic_matrix, extrinsic_matrix)

            if end_2d is not None:
                # ç»˜åˆ¶ç®­å¤´
                cv2.arrowedLine(image, (u_orig, v_orig), tuple(end_2d), color, 3)

                # æ·»åŠ æ ‡ç­¾
                label_pos = (end_2d[0] + 5, end_2d[1])
                cv2.putText(image, label, label_pos, cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

                # æ·»åŠ é»‘è‰²è¾¹æ¡†æé«˜å¯è¯»æ€§
                cv2.putText(image, label, (label_pos[0] + 1, label_pos[1] + 1),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 3)

        # ç»˜åˆ¶åŸç‚¹
        cv2.circle(image, (u_orig, v_orig), 5, (255, 255, 255), -1)
        cv2.circle(image, (u_orig, v_orig), 8, (0, 0, 0), 2)

        return True

    except Exception as e:
        print(f"ç»˜åˆ¶çœŸå®åæ ‡ç³»å¤±è´¥: {e}")
        return False

def draw_coordinate_frame_simple(image, origin_2d, axis_length=50):
    """
    ç®€åŒ–çš„åæ ‡ç³»ç»˜åˆ¶ (å½“æ— æ³•è¿›è¡Œ3DæŠ•å½±æ—¶ä½¿ç”¨)
    æ ¹æ®å¸¸è§çš„ç›¸æœºè§†è§’è¿›è¡Œè¿‘ä¼¼ç»˜åˆ¶
    """
    u, v = origin_2d

    # æ ¹æ®ç¬¬ä¸‰äººç§°è§†è§’çš„å¸¸è§æƒ…å†µè¿›è¡Œç»˜åˆ¶
    # Xè½´ - çº¢è‰²: å‘å³å‰æ–¹ (è¿‘ä¼¼)
    x_end = (u + int(axis_length * 0.9), v + int(axis_length * 0.1))
    cv2.arrowedLine(image, (u, v), x_end, (0, 0, 255), 3)
    cv2.putText(image, 'X(å‰)', (x_end[0] + 5, x_end[1]), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)

    # Yè½´ - ç»¿è‰²: å‘å·¦ (è¿‘ä¼¼)
    y_end = (u - int(axis_length * 0.8), v + int(axis_length * 0.2))
    cv2.arrowedLine(image, (u, v), y_end, (0, 255, 0), 3)
    cv2.putText(image, 'Y(å·¦)', (y_end[0] - 30, y_end[1]), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

    # Zè½´ - è“è‰²: å‘ä¸Š
    z_end = (u, v - axis_length)
    cv2.arrowedLine(image, (u, v), z_end, (255, 0, 0), 3)
    cv2.putText(image, 'Z(ä¸Š)', (z_end[0] + 5, z_end[1] - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)

    # ç»˜åˆ¶åŸç‚¹
    cv2.circle(image, (u, v), 5, (255, 255, 255), -1)
    cv2.circle(image, (u, v), 8, (0, 0, 0), 2)

def draw_pose_annotation(image, position_2d, quaternion, gripper_width):
    """åœ¨å›¾åƒä¸Šç»˜åˆ¶ä½å§¿æ ‡æ³¨"""
    u, v = position_2d

    # ç»˜åˆ¶å¤¹çˆªä½ç½®ç‚¹
    cv2.circle(image, (u, v), 8, (0, 255, 255), -1)  # é»„è‰²åœ†ç‚¹
    cv2.circle(image, (u, v), 12, (0, 0, 0), 2)      # é»‘è‰²è¾¹æ¡†

    # ç»˜åˆ¶ä½å§¿ä¿¡æ¯æ–‡æœ¬
    info_lines = [
        f"Pos: ({position_2d[0]}, {position_2d[1]})",
        f"Quat: w={quaternion[0]:.2f}",
        f"      xyz=({quaternion[1]:.2f},{quaternion[2]:.2f},{quaternion[3]:.2f})",
        f"Gripper: {gripper_width:.2f}"
    ]

    for i, text in enumerate(info_lines):
        y_offset = v - 10 + i * 15
        # ç™½è‰²æ–‡å­—ï¼Œé»‘è‰²è¾¹æ¡†
        cv2.putText(image, text, (u + 16, y_offset + 1),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 0), 2)
        cv2.putText(image, text, (u + 15, y_offset),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)

    return image

def draw_simple_object_marker(image, position_2d, obj_info):
    """ç»˜åˆ¶ç®€å•çš„ç‰©ä½“æ ‡è®°ç‚¹ï¼ˆé¿å…ä¸­æ–‡ä¹±ç ï¼‰"""
    u, v = position_2d
    color = obj_info.get('color', (0, 255, 0))  # é»˜è®¤ç»¿è‰²

    # ç»˜åˆ¶ç‰©ä½“ä½ç½®ç‚¹ - ç®€æ´é£æ ¼
    cv2.circle(image, (u, v), 8, color, -1)  # å½©è‰²å®å¿ƒåœ†
    cv2.circle(image, (u, v), 10, (255, 255, 255), 2)  # ç™½è‰²è¾¹æ¡†
    cv2.circle(image, (u, v), 12, (0, 0, 0), 1)       # é»‘è‰²å¤–è¾¹æ¡†

    # åªæ˜¾ç¤ºè‹±æ–‡æ ‡ç­¾ï¼Œé¿å…ä¸­æ–‡ä¹±ç 
    obj_type = obj_info.get('type', 'object')
    name = obj_info.get('name', 'unknown')

    # ç®€åŒ–æ ‡ç­¾
    if 'carrot' in name.lower() or 'source' in obj_type:
        label = "CARROT"
        label_color = (0, 165, 255)  # æ©™è‰²
    elif 'plate' in name.lower() or 'target' in obj_type:
        label = "PLATE"
        label_color = (255, 0, 0)  # è“è‰²
    else:
        label = "OBJECT"
        label_color = color

    # ç»˜åˆ¶æ ‡ç­¾
    label_x = u + 15
    label_y = v - 10

    # ç¡®ä¿æ ‡ç­¾ä¸è¶…å‡ºå›¾åƒè¾¹ç•Œ
    if label_x + len(label) * 8 > image.shape[1]:
        label_x = u - len(label) * 8 - 5
    if label_y < 20:
        label_y = v + 25

    # ç»˜åˆ¶æ ‡ç­¾èƒŒæ™¯
    text_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]
    cv2.rectangle(image, (label_x - 3, label_y - text_size[1] - 3),
                 (label_x + text_size[0] + 3, label_y + 3), (0, 0, 0), -1)

    # ç»˜åˆ¶æ ‡ç­¾æ–‡å­—
    cv2.putText(image, label, (label_x, label_y),
               cv2.FONT_HERSHEY_SIMPLEX, 0.5, label_color, 2)

    return image

def get_robot_pose_and_image_from_env(env, obs):
    """ä»ç°æœ‰ç¯å¢ƒå’Œè§‚å¯Ÿä¸­è·å–ä½å§¿å’Œå›¾åƒæ•°æ®"""
    if "extra" in obs and "tcp_pose" in obs["extra"]:
        tcp_pose = obs["extra"]["tcp_pose"]
        position = tcp_pose[:3]        # ä½ç½® (x, y, z)
        quaternion = tcp_pose[3:7]     # å››å…ƒæ•° (w, x, y, z)

        # è·å–å¤¹çˆªçŠ¶æ€
        gripper_width = 0.5
        try:
            agent = env.unwrapped.agent if hasattr(env, 'unwrapped') else env.agent
            if hasattr(agent, 'gripper_closedness'):
                closedness = agent.gripper_closedness
                gripper_width = max(0.0, min(1.0, 1.0 - closedness))
        except Exception:
            pass

        # è·å–å›¾åƒ
        image = None
        camera_name = "unknown"

        if "image" in obs:
            cam_imgs = obs["image"]
            if "3rd_view_camera" in cam_imgs and "rgb" in cam_imgs["3rd_view_camera"]:
                image = cam_imgs["3rd_view_camera"]["rgb"]
                camera_name = "3rd_view_camera"
            elif "base_camera" in cam_imgs and "rgb" in cam_imgs["base_camera"]:
                image = cam_imgs["base_camera"]["rgb"]
                camera_name = "base_camera"

        if image is None:
            image = get_image_from_maniskill2_obs_dict(env, obs)
            camera_name = "default"

        # è·å–ç›¸æœºå‚æ•°
        camera_params = obs.get("camera_param", None)

        # è·å–ç‰©ä½“åæ ‡ä¿¡æ¯
        objects_info = get_object_coordinates_from_env(env, obs)

        return {
            'position': position,
            'quaternion': quaternion,
            'gripper_width': gripper_width,
            'image': image,
            'camera_params': camera_params,
            'camera_name': camera_name,
            'objects_info': objects_info
        }
    else:
        raise KeyError(f"æœªåœ¨obsä¸­æ‰¾åˆ°tcp_pose")

def get_object_coordinates_from_env(env, obs, reset_info=None):
    """ä»ç¯å¢ƒä¸­è·å–ç›®æ ‡ç‰©ä½“çš„3Dåæ ‡ä¿¡æ¯"""
    objects_info = {}

    try:
        unwrapped_env = env.unwrapped if hasattr(env, 'unwrapped') else env

        # æ–¹æ³•1: ä»ç¯å¢ƒå±æ€§è·å–ç›®æ ‡ç‰©ä½“å½“å‰ä½å§¿
        if hasattr(unwrapped_env, 'target_obj_pose'):
            print("ä»ç¯å¢ƒå±æ€§è·å–ç›®æ ‡ç‰©ä½“å½“å‰ä½å§¿")
            target_pose = unwrapped_env.target_obj_pose

            # å¤„ç†ä¸åŒçš„ä½å§¿æ ¼å¼
            position = None
            quaternion = None

            if hasattr(target_pose, 'p') and hasattr(target_pose, 'q'):
                # Sapien Poseå¯¹è±¡
                position = np.array(target_pose.p)
                # å¤„ç†å››å…ƒæ•° - å¯èƒ½æ˜¯å¯¹è±¡æˆ–numpyæ•°ç»„
                if hasattr(target_pose.q, 'w'):
                    quaternion = np.array([target_pose.q.w, target_pose.q.x, target_pose.q.y, target_pose.q.z])
                else:
                    # å‡è®¾æ˜¯numpyæ•°ç»„æ ¼å¼ [w, x, y, z]
                    quaternion = np.array(target_pose.q)
            elif isinstance(target_pose, np.ndarray) and len(target_pose) >= 7:
                # numpyæ•°ç»„æ ¼å¼ [x, y, z, qw, qx, qy, qz]
                position = target_pose[:3]
                quaternion = target_pose[3:7]
            elif hasattr(target_pose, '__len__') and len(target_pose) >= 7:
                # åˆ—è¡¨æˆ–å…¶ä»–åºåˆ—æ ¼å¼
                position = np.array(target_pose[:3])
                quaternion = np.array(target_pose[3:7])

            if position is not None and quaternion is not None:
                target_name = getattr(unwrapped_env, 'episode_target_obj_name', 'target_plate')
                objects_info['target_object_current'] = {
                    'name': target_name,
                    'position': position,
                    'quaternion': quaternion,
                    'type': 'target_current',
                    'color': (255, 0, 255),  # ç´«è‰² - ç›®æ ‡ç‰©ä½“å½“å‰ä½ç½®
                    'description': f'Target({target_name}) current'
                }

        # æ–¹æ³•2: è·å–æºç‰©ä½“ï¼ˆå¦‚èƒ¡èåœï¼‰å½“å‰ä½ç½®
        if hasattr(unwrapped_env, 'episode_target_obj'):
            print("2.è·å–æºç‰©ä½“ï¼ˆå¦‚èƒ¡èåœï¼‰å½“å‰ä½ç½®")
            target_obj = unwrapped_env.episode_target_obj
            if hasattr(target_obj, 'get_pose'):
                pose = target_obj.get_pose()
                position = np.array(pose.p)
                # å¤„ç†å››å…ƒæ•° - å¯èƒ½æ˜¯å¯¹è±¡æˆ–numpyæ•°ç»„
                if hasattr(pose.q, 'w'):
                    quaternion = np.array([pose.q.w, pose.q.x, pose.q.y, pose.q.z])
                else:
                    # å‡è®¾æ˜¯numpyæ•°ç»„æ ¼å¼ [w, x, y, z]
                    quaternion = np.array(pose.q)
                obj_name = getattr(target_obj, 'name', 'target_object')
                objects_info['episode_target_obj'] = {
                    'name': obj_name,
                    'position': position,
                    'quaternion': quaternion,
                    'type': 'episode_target',
                    'color': (0, 255, 255),  # é’è‰² - episodeç›®æ ‡ç‰©ä½“
                    'description': f'Episodeç›®æ ‡ç‰©ä½“({obj_name})'
                }

        # æ–¹æ³•3: ä»reset_infoè·å–ç‰©ä½“åˆå§‹ä½å§¿
        if reset_info is None:
            print("3.ä»reset_infoè·å–ç‰©ä½“åˆå§‹ä½å§¿")
            # å°è¯•ä»ç¯å¢ƒè·å–reset_info
            if hasattr(unwrapped_env, '_last_reset_info'):
                reset_info = unwrapped_env._last_reset_info
            else:
                reset_info = getattr(unwrapped_env, 'reset_info', None)

        if reset_info and isinstance(reset_info, dict):
            # è·å–æºç‰©ä½“ï¼ˆèƒ¡èåœï¼‰åˆå§‹ä½ç½®
            if 'episode_source_obj_init_pose_wrt_robot_base' in reset_info:
                source_pose = reset_info['episode_source_obj_init_pose_wrt_robot_base']

                # å¤„ç†ä¸åŒçš„ä½å§¿æ ¼å¼
                position = None
                quaternion = None

                if hasattr(source_pose, 'p') and hasattr(source_pose, 'q'):
                    # Sapien Poseå¯¹è±¡
                    position = np.array(source_pose.p)
                    # å¤„ç†å››å…ƒæ•° - å¯èƒ½æ˜¯å¯¹è±¡æˆ–numpyæ•°ç»„
                    if hasattr(source_pose.q, 'w'):
                        quaternion = np.array([source_pose.q.w, source_pose.q.x, source_pose.q.y, source_pose.q.z])
                    else:
                        # å‡è®¾æ˜¯numpyæ•°ç»„æ ¼å¼ [w, x, y, z]
                        quaternion = np.array(source_pose.q)
                elif isinstance(source_pose, np.ndarray) and len(source_pose) >= 7:
                    # numpyæ•°ç»„æ ¼å¼ [x, y, z, qw, qx, qy, qz]
                    position = source_pose[:3]
                    quaternion = source_pose[3:7]
                elif hasattr(source_pose, '__len__') and len(source_pose) >= 7:
                    # åˆ—è¡¨æˆ–å…¶ä»–åºåˆ—æ ¼å¼
                    position = np.array(source_pose[:3])
                    quaternion = np.array(source_pose[3:7])

                if position is not None and quaternion is not None:
                    source_name = reset_info.get('episode_source_obj_name', 'carrot')
                    objects_info['source_object_init'] = {
                        'name': source_name,
                        'position': position,
                        'quaternion': quaternion,
                        'type': 'source_init',
                        'color': (0, 165, 255),  # æ©™è‰² - æºç‰©ä½“åˆå§‹ä½ç½®
                        'description': f'Source({source_name}) init'
                    }

            # è·å–ç›®æ ‡ç‰©ä½“ï¼ˆç›˜å­ï¼‰åˆå§‹ä½ç½®
            if 'episode_target_obj_init_pose_wrt_robot_base' in reset_info:
                target_pose = reset_info['episode_target_obj_init_pose_wrt_robot_base']

                # å¤„ç†ä¸åŒçš„ä½å§¿æ ¼å¼
                position = None
                quaternion = None

                if hasattr(target_pose, 'p') and hasattr(target_pose, 'q'):
                    # Sapien Poseå¯¹è±¡
                    position = np.array(target_pose.p)
                    # å¤„ç†å››å…ƒæ•° - å¯èƒ½æ˜¯å¯¹è±¡æˆ–numpyæ•°ç»„
                    if hasattr(target_pose.q, 'w'):
                        quaternion = np.array([target_pose.q.w, target_pose.q.x, target_pose.q.y, target_pose.q.z])
                    else:
                        # å‡è®¾æ˜¯numpyæ•°ç»„æ ¼å¼ [w, x, y, z]
                        quaternion = np.array(target_pose.q)
                elif isinstance(target_pose, np.ndarray) and len(target_pose) >= 7:
                    # numpyæ•°ç»„æ ¼å¼ [x, y, z, qw, qx, qy, qz]
                    position = target_pose[:3]
                    quaternion = target_pose[3:7]
                elif hasattr(target_pose, '__len__') and len(target_pose) >= 7:
                    # åˆ—è¡¨æˆ–å…¶ä»–åºåˆ—æ ¼å¼
                    position = np.array(target_pose[:3])
                    quaternion = np.array(target_pose[3:7])

                if position is not None and quaternion is not None:
                    target_name = reset_info.get('episode_target_obj_name', 'plate')
                    objects_info['target_object_init'] = {
                        'name': target_name,
                        'position': position,
                        'quaternion': quaternion,
                        'type': 'target_init',
                        'color': (255, 0, 0),  # è“è‰² - ç›®æ ‡ç‰©ä½“åˆå§‹ä½ç½®
                        'description': f'Target({target_name}) init'
                    }

        # æ–¹æ³•4: å°è¯•é€šè¿‡åœºæ™¯è·å–æ‰€æœ‰ç›¸å…³ç‰©ä½“
        if hasattr(unwrapped_env, 'scene'):
            print("4.å°è¯•é€šè¿‡åœºæ™¯è·å–æ‰€æœ‰ç›¸å…³ç‰©ä½“")
            scene = unwrapped_env.scene
            if hasattr(scene, 'get_all_actors'):
                actors = scene.get_all_actors()
                for actor in actors:
                    name = getattr(actor, 'name', '')
                    # æŸ¥æ‰¾åŒ…å«å…³é”®è¯çš„ç‰©ä½“
                    if any(keyword in name.lower() for keyword in ['carrot', 'plate', 'target', 'obj']):
                        try:
                            pose = actor.get_pose()
                            position = np.array(pose.p)
                            # å¤„ç†å››å…ƒæ•° - å¯èƒ½æ˜¯å¯¹è±¡æˆ–numpyæ•°ç»„
                            if hasattr(pose.q, 'w'):
                                quaternion = np.array([pose.q.w, pose.q.x, pose.q.y, pose.q.z])
                            else:
                                # å‡è®¾æ˜¯numpyæ•°ç»„æ ¼å¼ [w, x, y, z]
                                quaternion = np.array(pose.q)
                            objects_info[f'scene_object_{name}'] = {
                                'name': name,
                                'position': position,
                                'quaternion': quaternion,
                                'type': 'scene_object',
                                'color': (0, 255, 0),  # ç»¿è‰² - åœºæ™¯ç‰©ä½“
                                'description': f'åœºæ™¯ç‰©ä½“({name})'
                            }
                        except Exception:
                            continue

        return objects_info

    except Exception as e:
        print(f"è·å–ç‰©ä½“åæ ‡å¤±è´¥: {e}")
        return {}

def draw_simple_gripper_marker(image, position_2d):
    """ç»˜åˆ¶ç®€å•çš„å¤¹çˆªæ ‡è®°ç‚¹ï¼ˆæ— å¤æ‚åæ ‡ç³»ï¼‰"""
    u, v = position_2d

    # ç»˜åˆ¶ç®€å•çš„å¤¹çˆªæ ‡è®°
    cv2.circle(image, (u, v), 10, (0, 255, 255), -1)  # é’è‰²å®å¿ƒåœ†
    cv2.circle(image, (u, v), 12, (255, 255, 255), 2)  # ç™½è‰²è¾¹æ¡†
    cv2.circle(image, (u, v), 14, (0, 0, 0), 1)       # é»‘è‰²å¤–è¾¹æ¡†

    # ç®€å•æ ‡ç­¾
    label = "GRIPPER"
    label_color = (0, 255, 255)  # é’è‰²

    # ç»˜åˆ¶æ ‡ç­¾
    label_x = u + 18
    label_y = v - 12

    # ç¡®ä¿æ ‡ç­¾ä¸è¶…å‡ºå›¾åƒè¾¹ç•Œ
    if label_x + len(label) * 8 > image.shape[1]:
        label_x = u - len(label) * 8 - 5
    if label_y < 20:
        label_y = v + 30

    # ç»˜åˆ¶æ ‡ç­¾èƒŒæ™¯
    text_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]
    cv2.rectangle(image, (label_x - 3, label_y - text_size[1] - 3),
                 (label_x + text_size[0] + 3, label_y + 3), (0, 0, 0), -1)

    # ç»˜åˆ¶æ ‡ç­¾æ–‡å­—
    cv2.putText(image, label, (label_x, label_y),
               cv2.FONT_HERSHEY_SIMPLEX, 0.5, label_color, 2)

    return image

def create_simple_annotated_image(data, step_name, save_name=None, include_objects=True):
    """åˆ›å»ºç®€åŒ–çš„æ ‡æ³¨å›¾åƒï¼ˆæ— å¤æ‚åæ ‡ç³»ï¼‰"""
    image = data['image'].copy()
    h, w = image.shape[:2]

    # è·å–ç›¸æœºå‚æ•°
    intrinsic = None
    extrinsic = None
    use_real_projection = False

    if data['camera_params'] and data['camera_name'] in data['camera_params']:
        try:
            params = data['camera_params'][data['camera_name']]
            intrinsic = params['intrinsic_cv']
            extrinsic = params['extrinsic_cv']
            use_real_projection = True
        except Exception:
            pass

    # 1. å¤„ç†å¤¹çˆªä½å§¿æ ‡æ³¨ - ç®€åŒ–ç‰ˆæœ¬
    gripper_position_2d = None
    if use_real_projection:
        gripper_position_2d = project_3d_to_2d(
            data['position'], intrinsic, extrinsic
        )

    # å¦‚æœæŠ•å½±å¤±è´¥ï¼Œä½¿ç”¨å›¾åƒä¸­å¿ƒ
    if gripper_position_2d is None:
        gripper_position_2d = [w // 2, h // 2]

    # æ£€æŸ¥ä½ç½®æ˜¯å¦åœ¨å›¾åƒèŒƒå›´å†…
    if not (0 <= gripper_position_2d[0] < w and 0 <= gripper_position_2d[1] < h):
        gripper_position_2d = [w // 2, h // 2]

    # ç»˜åˆ¶æ ‡æ³¨
    annotated_image = image.copy()

    # ç»˜åˆ¶å¤¹çˆªæœå‘ç®­å¤´ï¼ˆç»Ÿä¸€é£æ ¼ï¼‰
    if use_real_projection:
        success = draw_affordance_arrow(
            annotated_image, gripper_position_2d, data['position'], data['quaternion'],
            intrinsic, extrinsic, arrow_length=0.08, arrow_color=(0, 255, 0),
            arrow_thickness=3, show_point=True
        )
        if not success:
            draw_affordance_arrow_simple(
                annotated_image, gripper_position_2d, data['quaternion'],
                arrow_length=60, arrow_color=(0, 255, 0), arrow_thickness=3, show_point=True
            )
    else:
        draw_affordance_arrow_simple(
            annotated_image, gripper_position_2d, data['quaternion'],
            arrow_length=60, arrow_color=(0, 255, 0), arrow_thickness=3, show_point=True
        )

    # ç»˜åˆ¶å¤¹çˆªä½å§¿ä¿¡æ¯
    annotated_image = draw_pose_annotation(
        annotated_image, gripper_position_2d,
        data['quaternion'], data['gripper_width']
    )

    # 2. å¤„ç†ç‰©ä½“æ ‡æ³¨ - ç®€åŒ–ç‰ˆæœ¬
    if include_objects and 'objects_info' in data:
        objects_info = data['objects_info']

        for obj_key, obj_info in objects_info.items():
            obj_position_3d = obj_info['position']

            # æŠ•å½±ç‰©ä½“ä½ç½®åˆ°2D
            obj_position_2d = None
            if use_real_projection:
                obj_position_2d = project_3d_to_2d(
                    obj_position_3d, intrinsic, extrinsic
                )

            # å¦‚æœæŠ•å½±å¤±è´¥æˆ–ä½ç½®è¶…å‡ºèŒƒå›´ï¼Œè·³è¿‡æ­¤ç‰©ä½“
            if (obj_position_2d is None or
                not (0 <= obj_position_2d[0] < w and 0 <= obj_position_2d[1] < h)):
                continue

            # ç»˜åˆ¶ç®€åŒ–çš„ç‰©ä½“æ ‡è®°
            draw_simple_object_marker(annotated_image, obj_position_2d, obj_info)

    # 3. æ·»åŠ æ­¥éª¤æ ‡ç­¾
    cv2.putText(annotated_image, f"{step_name}", (10, 30),
               cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
    cv2.putText(annotated_image, f"{step_name}", (11, 31),
               cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 3)

    # 4. æ·»åŠ ç‰©ä½“æ•°é‡ä¿¡æ¯
    if include_objects and 'objects_info' in data:
        obj_count = len(data['objects_info'])
        obj_text = f"Objects: {obj_count}"
        cv2.putText(annotated_image, obj_text, (10, h - 20),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        cv2.putText(annotated_image, obj_text, (11, h - 19),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)

    # 5. ä¿å­˜å›¾åƒ
    if save_name:
        cv2.imwrite(save_name, cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))

    return annotated_image

def create_annotated_image(data, step_name, save_name=None, include_objects=True):
    """åˆ›å»ºå•ä¸ªæ ‡æ³¨å›¾åƒï¼Œæ”¯æŒå¤¹çˆªå’Œç‰©ä½“æ ‡æ³¨"""
    image = data['image'].copy()
    h, w = image.shape[:2]

    # è·å–ç›¸æœºå‚æ•°
    intrinsic = None
    extrinsic = None
    use_real_projection = False

    if data['camera_params'] and data['camera_name'] in data['camera_params']:
        try:
            params = data['camera_params'][data['camera_name']]
            intrinsic = params['intrinsic_cv']
            extrinsic = params['extrinsic_cv']
            use_real_projection = True
        except Exception:
            pass

    # 1. å¤„ç†å¤¹çˆªä½å§¿æ ‡æ³¨
    gripper_position_2d = None
    if use_real_projection:
        gripper_position_2d = project_3d_to_2d(
            data['position'], intrinsic, extrinsic
        )

    # å¦‚æœæŠ•å½±å¤±è´¥ï¼Œä½¿ç”¨å›¾åƒä¸­å¿ƒ
    if gripper_position_2d is None:
        gripper_position_2d = [w // 2, h // 2]

    # æ£€æŸ¥ä½ç½®æ˜¯å¦åœ¨å›¾åƒèŒƒå›´å†…
    if not (0 <= gripper_position_2d[0] < w and 0 <= gripper_position_2d[1] < h):
        gripper_position_2d = [w // 2, h // 2]

    # ç»˜åˆ¶æ ‡æ³¨
    annotated_image = image.copy()

    # ç»˜åˆ¶å¤¹çˆªæœå‘ç®­å¤´ï¼ˆç»Ÿä¸€é£æ ¼ï¼‰
    if use_real_projection:
        success = draw_affordance_arrow(
            annotated_image, gripper_position_2d, data['position'], data['quaternion'],
            intrinsic, extrinsic, arrow_length=0.08, arrow_color=(0, 255, 0),
            arrow_thickness=3, show_point=True
        )
        if not success:
            draw_affordance_arrow_simple(
                annotated_image, gripper_position_2d, data['quaternion'],
                arrow_length=60, arrow_color=(0, 255, 0), arrow_thickness=3, show_point=True
            )
    else:
        draw_affordance_arrow_simple(
            annotated_image, gripper_position_2d, data['quaternion'],
            arrow_length=60, arrow_color=(0, 255, 0), arrow_thickness=3, show_point=True
        )

    # ç»˜åˆ¶å¤¹çˆªä½å§¿ä¿¡æ¯
    annotated_image = draw_pose_annotation(
        annotated_image, gripper_position_2d,
        data['quaternion'], data['gripper_width']
    )

    # 2. å¤„ç†ç‰©ä½“æ ‡æ³¨ - ç®€åŒ–ç‰ˆæœ¬
    if include_objects and 'objects_info' in data:
        objects_info = data['objects_info']

        for obj_key, obj_info in objects_info.items():
            obj_position_3d = obj_info['position']

            # æŠ•å½±ç‰©ä½“ä½ç½®åˆ°2D
            obj_position_2d = None
            if use_real_projection:
                obj_position_2d = project_3d_to_2d(
                    obj_position_3d, intrinsic, extrinsic
                )

            # å¦‚æœæŠ•å½±å¤±è´¥æˆ–ä½ç½®è¶…å‡ºèŒƒå›´ï¼Œè·³è¿‡æ­¤ç‰©ä½“
            if (obj_position_2d is None or
                not (0 <= obj_position_2d[0] < w and 0 <= obj_position_2d[1] < h)):
                continue

            # ç»˜åˆ¶ç®€åŒ–çš„ç‰©ä½“æ ‡è®°
            draw_simple_object_marker(annotated_image, obj_position_2d, obj_info)

    # 3. æ·»åŠ æ­¥éª¤æ ‡ç­¾
    cv2.putText(annotated_image, f"{step_name}", (10, 30),
               cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
    cv2.putText(annotated_image, f"{step_name}", (11, 31),
               cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 3)

    # 4. æ·»åŠ å¤¹çˆª3Dä½ç½®ä¿¡æ¯
    pos_text = f"Gripper 3D: ({data['position'][0]:.3f}, {data['position'][1]:.3f}, {data['position'][2]:.3f})"
    cv2.putText(annotated_image, pos_text, (10, h - 20),
               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
    cv2.putText(annotated_image, pos_text, (11, h - 19),
               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)

    # 5. æ·»åŠ ç‰©ä½“æ•°é‡ä¿¡æ¯
    if include_objects and 'objects_info' in data:
        obj_count = len(data['objects_info'])
        obj_text = f"Objects: {obj_count}"
        cv2.putText(annotated_image, obj_text, (10, h - 40),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        cv2.putText(annotated_image, obj_text, (11, h - 39),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)

    # 6. ä¿å­˜å›¾åƒ
    if save_name:
        cv2.imwrite(save_name, cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))

    return annotated_image

def test_multiple_gripper_positions():
    """æµ‹è¯•å¤šä¸ªä¸åŒçš„å¤¹çˆªä½ç½®"""
    print("=== æµ‹è¯•ä¸åŒå¤¹çˆªä½ç½®çš„æ ‡æ³¨æ•ˆæœ ===\n")

    task_name = "widowx_carrot_on_plate"
    env = simpler_env.make(task_name)

    try:
        # åˆå§‹çŠ¶æ€
        obs, reset_info = env.reset(seed=42)
        initial_data = get_robot_pose_and_image_from_env(env, obs)

        # æ£€æŸ¥åŠ¨ä½œç©ºé—´
        print("ç¯å¢ƒä¿¡æ¯:")
        print(f"   åŠ¨ä½œç©ºé—´: {env.action_space}")
        print(f"   åŠ¨ä½œç©ºé—´ç»´åº¦: {env.action_space.shape}")

        print("\n1. åˆå§‹ä½ç½®:")
        print(f"   3Dä½ç½®: {initial_data['position']}")

        # å®šä¹‰å¤šä¸ªæµ‹è¯•åŠ¨ä½œ (7ç»´åŠ¨ä½œå‘é‡: [x, y, z, rx, ry, rz, gripper])
        # ä½¿ç”¨è¾ƒå°çš„åŠ¨ä½œå€¼ä»¥é¿å…æ®µé”™è¯¯
        test_actions = [
            {
                'name': 'åˆå§‹ä½ç½®',
                'action': None,
                'description': 'ç¯å¢ƒé‡ç½®åçš„åˆå§‹ä½ç½®'
            },
            {
                'name': 'å‘ä¸Šç§»åŠ¨',
                'action': np.array([0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0], dtype=np.float32),
                'description': 'å¤¹çˆªå‘ä¸Šç§»åŠ¨5cm'
            },
            {
                'name': 'å‘å³ç§»åŠ¨',
                'action': np.array([0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0], dtype=np.float32),
                'description': 'å¤¹çˆªå‘å³ç§»åŠ¨5cm'
            },
            {
                'name': 'å‘å‰ç§»åŠ¨',
                'action': np.array([0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], dtype=np.float32),
                'description': 'å¤¹çˆªå‘å‰ç§»åŠ¨5cm'
            },
            {
                'name': 'å‘å·¦ç§»åŠ¨',
                'action': np.array([0.0, -0.05, 0.0, 0.0, 0.0, 0.0, 0.0], dtype=np.float32),
                'description': 'å¤¹çˆªå‘å·¦ç§»åŠ¨5cm'
            },
            {
                'name': 'è½»å¾®æ—‹è½¬',
                'action': np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0], dtype=np.float32),
                'description': 'å¤¹çˆªè½»å¾®æ—‹è½¬'
            }
        ]

        annotated_images = []
        position_data = []

        # å¤„ç†åˆå§‹ä½ç½®
        initial_annotated = create_annotated_image(initial_data, test_actions[0]['name'],
                                                   'gripper_pos_0_initial.png')
        annotated_images.append(initial_annotated)
        position_data.append({
            'name': test_actions[0]['name'],
            'position': initial_data['position'].copy(),
            'quaternion': initial_data['quaternion'].copy()
        })

        print(f"   ä¿å­˜: gripper_pos_0_initial.png")

        # æ‰§è¡Œæ¯ä¸ªåŠ¨ä½œ
        for i, action_info in enumerate(test_actions[1:], 1):
            print(f"\n{i+1}. {action_info['description']}:")

            try:
                # æ‰§è¡ŒåŠ¨ä½œ
                action = action_info['action']
                print(f"   åŠ¨ä½œå‘é‡ç»´åº¦: {action.shape}")
                print(f"   åŠ¨ä½œå‘é‡: {action}")
                print(f"   åŠ¨ä½œèŒƒå›´æ£€æŸ¥: min={action.min():.3f}, max={action.max():.3f}")

                # ç¡®ä¿åŠ¨ä½œåœ¨åˆæ³•èŒƒå›´å†…
                action = np.clip(action, env.action_space.low, env.action_space.high)

                print("   æ‰§è¡Œç¯å¢ƒæ­¥è¿›...")
                obs, reward, terminated, truncated, info = env.step(action)
                print("   âœ“ ç¯å¢ƒæ­¥è¿›æˆåŠŸ")

                # è·å–æ–°çš„ä½å§¿æ•°æ®
                print("   è·å–æ–°ä½å§¿æ•°æ®...")
                new_data = get_robot_pose_and_image_from_env(env, obs)

                print(f"   æ–°3Dä½ç½®: {new_data['position']}")
                print(f"   ä½ç½®å˜åŒ–: {new_data['position'] - initial_data['position']}")

                # åˆ›å»ºæ ‡æ³¨å›¾åƒ
                filename = f'gripper_pos_{i}_{action_info["name"].replace(" ", "_").lower()}.png'
                print(f"   åˆ›å»ºæ ‡æ³¨å›¾åƒ: {filename}")
                annotated_img = create_annotated_image(new_data, action_info['name'], filename)
                annotated_images.append(annotated_img)

                position_data.append({
                    'name': action_info['name'],
                    'position': new_data['position'].copy(),
                    'quaternion': new_data['quaternion'].copy()
                })

                print(f"   âœ“ ä¿å­˜: {filename}")

                # æ£€æŸ¥æ˜¯å¦éœ€è¦ç»ˆæ­¢
                if terminated or truncated:
                    print("   âš  ç¯å¢ƒæŒ‡ç¤ºç»ˆæ­¢ï¼Œé‡ç½®ç¯å¢ƒ...")
                    obs, reset_info = env.reset(seed=42)

            except Exception as e:
                print(f"   âŒ æ‰§è¡ŒåŠ¨ä½œå¤±è´¥: {e}")
                print("   å°è¯•é‡ç½®ç¯å¢ƒ...")
                try:
                    obs, reset_info = env.reset(seed=42)
                    print("   âœ“ ç¯å¢ƒé‡ç½®æˆåŠŸ")
                except Exception as reset_e:
                    print(f"   âŒ ç¯å¢ƒé‡ç½®ä¹Ÿå¤±è´¥: {reset_e}")
                    print("   è·³è¿‡æ­¤åŠ¨ä½œï¼Œç»§ç»­ä¸‹ä¸€ä¸ª...")
                    continue

        # åˆ›å»ºç½‘æ ¼å¯¹æ¯”å›¾
        print(f"\n=== åˆ›å»ºå¯¹æ¯”å›¾ ({len(annotated_images)} å¼ å›¾åƒ) ===")

        # è®¡ç®—ç½‘æ ¼å¸ƒå±€
        num_images = len(annotated_images)
        cols = 4  # æ¯è¡Œ4å¼ å›¾
        rows = (num_images + cols - 1) // cols

        if num_images > 0:
            h, w = annotated_images[0].shape[:2]

            # åˆ›å»ºç½‘æ ¼å›¾åƒ
            grid_h = h * rows
            grid_w = w * cols
            grid_image = np.zeros((grid_h, grid_w, 3), dtype=np.uint8)

            for i, img in enumerate(annotated_images):
                row = i // cols
                col = i % cols

                y_start = row * h
                y_end = y_start + h
                x_start = col * w
                x_end = x_start + w

                grid_image[y_start:y_end, x_start:x_end] = img

            # ä¿å­˜ç½‘æ ¼å¯¹æ¯”å›¾
            cv2.imwrite('gripper_positions_grid_comparison.png',
                       cv2.cvtColor(grid_image, cv2.COLOR_RGB2BGR))
            print("ä¿å­˜ç½‘æ ¼å¯¹æ¯”å›¾: gripper_positions_grid_comparison.png")

        # åˆ›å»ºæ°´å¹³å¯¹æ¯”å›¾ï¼ˆé€‰æ‹©å‰6å¼ ï¼‰
        if len(annotated_images) >= 2:
            selected_count = min(6, len(annotated_images))
            selected_images = annotated_images[:selected_count]

            # è°ƒæ•´å›¾åƒå¤§å°
            target_h = 300  # ç»Ÿä¸€é«˜åº¦
            resized_images = []
            for img in selected_images:
                h_orig, w_orig = img.shape[:2]
                target_w = int(w_orig * target_h / h_orig)
                resized = cv2.resize(img, (target_w, target_h))
                resized_images.append(resized)

            # æ°´å¹³æ‹¼æ¥
            comparison_horizontal = np.hstack(resized_images)
            cv2.imwrite('gripper_positions_horizontal.png',
                       cv2.cvtColor(comparison_horizontal, cv2.COLOR_RGB2BGR))
            print("ä¿å­˜æ°´å¹³å¯¹æ¯”å›¾: gripper_positions_horizontal.png")

        # æ‰“å°ä½ç½®ç»Ÿè®¡
        print(f"\n=== ä½ç½®å˜åŒ–ç»Ÿè®¡ ===")
        initial_pos = position_data[0]['position']
        for i, data in enumerate(position_data):
            if i == 0:
                print(f"{i+1}. {data['name']}: åŸºå‡†ä½ç½® {data['position']}")
            else:
                diff = data['position'] - initial_pos
                distance = np.linalg.norm(diff)
                print(f"{i+1}. {data['name']}: ä½ç§» {diff} (è·ç¦»: {distance:.3f}m)")

        print(f"\n=== æµ‹è¯•å®Œæˆ ===")
        print("ç”Ÿæˆçš„æ–‡ä»¶:")
        for i, action_info in enumerate(test_actions):
            if i == 0:
                print(f"  - gripper_pos_0_initial.png: {action_info['description']}")
            else:
                filename = f'gripper_pos_{i}_{action_info["name"].replace(" ", "_").lower()}.png'
                print(f"  - {filename}: {action_info['description']}")
        print("  - gripper_positions_grid_comparison.png: ç½‘æ ¼å¯¹æ¯”å›¾")
        print("  - gripper_positions_horizontal.png: æ°´å¹³å¯¹æ¯”å›¾")

        return annotated_images

    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return None

    finally:
        env.close()

def annotate_pose_on_image():
    """ä¸»å‡½æ•°ï¼šåœ¨å›¾åƒä¸Šæ ‡æ³¨æœºæ¢°è‡‚ä½å§¿"""
    print("=== åœ¨å›¾åƒä¸Šæ ‡æ³¨æœºæ¢°è‡‚6Dä½å§¿ (åæ ‡ç³»ä¿®æ­£ç‰ˆ) ===\n")

    try:
        # 1. è·å–ä½å§¿å’Œå›¾åƒæ•°æ®
        data = get_robot_pose_and_image()

        # 2. å‡†å¤‡æ ‡æ³¨
        image = data['image'].copy()
        h, w = image.shape[:2]

        # 3. å°è¯•è¿›è¡Œ3Dåˆ°2DæŠ•å½±
        position_2d = None
        use_real_projection = False

        if data['camera_params'] and data['camera_name'] in data['camera_params']:
            print("\nâœ“ å°è¯•3Dåˆ°2DæŠ•å½±:")
            try:
                params = data['camera_params'][data['camera_name']]
                intrinsic = params['intrinsic_cv']
                extrinsic = params['extrinsic_cv']

                position_2d = project_3d_to_2d(
                    data['position'], intrinsic, extrinsic
                )

                if position_2d:
                    print(f"  æŠ•å½±æˆåŠŸ: å¤¹çˆªä½ç½® -> å›¾åƒåæ ‡ {position_2d}")
                    use_real_projection = True
                else:
                    print("  æŠ•å½±å¤±è´¥: å¤¹çˆªä¸åœ¨ç›¸æœºè§†é‡å†…")
            except Exception as e:
                print(f"  æŠ•å½±å¤±è´¥: {e}")

        # 4. å¦‚æœæŠ•å½±å¤±è´¥ï¼Œä½¿ç”¨å›¾åƒä¸­å¿ƒ
        if position_2d is None:
            position_2d = [w // 2, h // 2]
            print(f"\nâš  ä½¿ç”¨å›¾åƒä¸­å¿ƒä½œä¸ºæ ‡æ³¨ä½ç½®: {position_2d}")

        # 5. æ£€æŸ¥ä½ç½®æ˜¯å¦åœ¨å›¾åƒèŒƒå›´å†…
        if not (0 <= position_2d[0] < w and 0 <= position_2d[1] < h):
            position_2d = [w // 2, h // 2]
            print(f"âš  ä½ç½®è¶…å‡ºå›¾åƒèŒƒå›´ï¼Œè°ƒæ•´åˆ°å›¾åƒä¸­å¿ƒ")

        # 6. ç»˜åˆ¶æ ‡æ³¨
        print("\nâœ“ ç»˜åˆ¶ä½å§¿æ ‡æ³¨:")
        annotated_image = image.copy()

        # å°è¯•ç»˜åˆ¶çœŸå®çš„åæ ‡ç³»
        if use_real_projection:
            success = draw_coordinate_frame_correct(
                annotated_image, position_2d, data['position'], data['quaternion'],
                intrinsic, extrinsic
            )
            if success:
                print("  ä½¿ç”¨çœŸå®3DæŠ•å½±ç»˜åˆ¶åæ ‡ç³»")
            else:
                print("  çœŸå®æŠ•å½±å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–åæ ‡ç³»")
                draw_coordinate_frame_simple(annotated_image, position_2d)
        else:
            print("  ä½¿ç”¨ç®€åŒ–åæ ‡ç³»ç»˜åˆ¶")
            draw_coordinate_frame_simple(annotated_image, position_2d)

        # ç»˜åˆ¶ä½å§¿ä¿¡æ¯
        annotated_image = draw_pose_annotation(
            annotated_image, position_2d,
            data['quaternion'], data['gripper_width']
        )

        # 7. ä¿å­˜ç»“æœ
        output_path = 'robot_pose_annotation_corrected.png'
        cv2.imwrite(output_path, cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))
        print(f"  ä¿å­˜æ ‡æ³¨å›¾åƒ: {output_path}")

        # 8. åˆ›å»ºå¯¹æ¯”å›¾
        comparison_image = np.hstack([image, annotated_image])
        comparison_path = 'pose_annotation_comparison_corrected.png'
        cv2.imwrite(comparison_path, cv2.cvtColor(comparison_image, cv2.COLOR_RGB2BGR))
        print(f"  ä¿å­˜å¯¹æ¯”å›¾: {comparison_path}")

        # 9. æ¸…ç†
        data['env'].close()

        return annotated_image

    except Exception as e:
        print(f"âŒ æ ‡æ³¨è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return None

def main():
    """ä¸»å‡½æ•°"""
    print("æœºæ¢°è‡‚æœ«ç«¯6Dä½å§¿è·å–å’Œå›¾åƒæ ‡æ³¨æ¼”ç¤º (åæ ‡ç³»ä¿®æ­£ç‰ˆ)")
    print("=" * 60)

    print("\n=== åæ ‡ç³»çº¦å®šè¯´æ˜ ===")
    print("ROS/ManiSkillæ ‡å‡† (å³æ‰‹åæ ‡ç³»):")
    print("  - Xè½´ (çº¢è‰²): å‘å‰")
    print("  - Yè½´ (ç»¿è‰²): å‘å·¦")
    print("  - Zè½´ (è“è‰²): å‘ä¸Š")
    print("\næ³¨æ„: åœ¨2Då›¾åƒä¸­çš„æ˜¾ç¤ºä¼šæ ¹æ®ç›¸æœºè§’åº¦è€Œå˜åŒ–")

    print("\n=== é€‰æ‹©æµ‹è¯•æ¨¡å¼ ===")
    print("1. å•æ¬¡ä½å§¿æ ‡æ³¨ï¼ˆåŸå§‹åŠŸèƒ½ï¼‰")
    print("2. å¤šä½ç½®æµ‹è¯•ï¼ˆæ¨èï¼‰- æµ‹è¯•ä¸åŒæ‰‹æŠ“ä½ç½®çš„æ ‡æ³¨æ•ˆæœ")
    print("3. å®‰å…¨æ¨¡å¼æµ‹è¯• - ä½¿ç”¨æå°åŠ¨ä½œå€¼é¿å…æ®µé”™è¯¯")
    print("4. ç‰©ä½“æ ‡æ³¨æµ‹è¯• - æµ‹è¯•ç‰©ä½“3Dåæ ‡è·å–å’Œæ ‡æ³¨åŠŸèƒ½")

    try:
        choice = input("\nè¯·é€‰æ‹©æµ‹è¯•æ¨¡å¼ (1, 2, 3 æˆ– 4ï¼Œé»˜è®¤ä¸º4): ").strip()
        if not choice:
            choice = "4"
    except:
        choice = "4"

    if choice == "1":
        print("\n=== æ‰§è¡Œå•æ¬¡ä½å§¿æ ‡æ³¨ ===")
        # è¿è¡Œå•æ¬¡ä½å§¿æ ‡æ³¨
        result = annotate_pose_on_image()

        if result is not None:
            print("\nâœ… ä½å§¿æ ‡æ³¨å®Œæˆï¼")
            print("\nç”Ÿæˆçš„æ–‡ä»¶ï¼š")
            print("  - robot_pose_annotation_corrected.png: ä¿®æ­£åæ ‡ç³»çš„æ ‡æ³¨å›¾åƒ")
            print("  - pose_annotation_comparison_corrected.png: åŸå§‹å›¾åƒä¸æ ‡æ³¨å›¾åƒå¯¹æ¯”")

            print("\n=== æ”¹è¿›è¯´æ˜ ===")
            print("1. âœ“ ä½¿ç”¨æ­£ç¡®çš„ROSåæ ‡ç³»çº¦å®š (Xå‰, Yå·¦, Zä¸Š)")
            print("2. âœ“ å°è¯•ä½¿ç”¨çœŸå®çš„3Dåˆ°2DæŠ•å½±ç»˜åˆ¶åæ ‡è½´")
            print("3. âœ“ æ ¹æ®å››å…ƒæ•°è®¡ç®—å®é™…çš„è½´æ–¹å‘")
            print("4. âœ“ æä¾›ç®€åŒ–ç‰ˆæœ¬ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ")
            print("5. âœ“ æ·»åŠ åæ ‡è½´æ ‡ç­¾è¯´æ˜æ–¹å‘")
        else:
            print("\nâŒ ä½å§¿æ ‡æ³¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒé…ç½®")

    elif choice == "2":
        print("\n=== æ‰§è¡Œå¤šä½ç½®æµ‹è¯• ===")
        # è¿è¡Œå¤šä½ç½®æµ‹è¯•
        results = test_multiple_gripper_positions()

        if results is not None:
            print("\nâœ… å¤šä½ç½®æµ‹è¯•å®Œæˆï¼")
            print("\n=== æµ‹è¯•æ€»ç»“ ===")
            print(f"âœ“ å…±æµ‹è¯•äº† {len(results)} ä¸ªä¸åŒçš„æ‰‹æŠ“ä½ç½®")
            print("âœ“ ç”Ÿæˆäº†å¯¹åº”çš„æ ‡æ³¨å›¾åƒ")
            print("âœ“ åˆ›å»ºäº†ç½‘æ ¼å’Œæ°´å¹³å¯¹æ¯”å›¾")
            print("âœ“ è¾“å‡ºäº†ä½ç½®å˜åŒ–ç»Ÿè®¡ä¿¡æ¯")

            print("\n=== è§‚å¯Ÿé‡ç‚¹ ===")
            print("1. ğŸ” è§‚å¯Ÿæ‰‹æŠ“ä½ç½®å˜åŒ–æ˜¯å¦å‡†ç¡®åæ˜ åœ¨å›¾åƒæ ‡æ³¨ä¸­")
            print("2. ğŸ” æ£€æŸ¥3Dåˆ°2DæŠ•å½±çš„å‡†ç¡®æ€§")
            print("3. ğŸ” éªŒè¯åæ ‡ç³»æ–¹å‘æ ‡æ³¨æ˜¯å¦æ­£ç¡®")
            print("4. ğŸ” ç¡®è®¤ä¸åŒä½ç½®ä¸‹çš„å§¿æ€å˜åŒ–")
            print("5. ğŸ” å¯¹æ¯”ç½‘æ ¼å›¾ä¸­çš„æ ‡æ³¨ä¸€è‡´æ€§")

            print("\nğŸ’¡ å»ºè®®: æŸ¥çœ‹ç”Ÿæˆçš„å¯¹æ¯”å›¾æ¥åˆ†ææ ‡æ³¨æ•ˆæœ")
        else:
            print("\nâŒ å¤šä½ç½®æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒé…ç½®")

    elif choice == "4":
        print("\n=== Simple Object Annotation Test ===")
        # è¿è¡Œç®€åŒ–çš„ç‰©ä½“æ ‡æ³¨æµ‹è¯•
        result = test_simple_object_annotation()

        if result is not None:
            print("\nâœ… Object annotation test completed!")
            print("\n=== Test Summary ===")
            print("âœ“ Successfully obtained 3D coordinates of objects")
            print("âœ“ Implemented 3D to 2D projection for objects")
            print("âœ“ Created images with object annotations")
            print("âœ“ Generated comparison images")

            print("\n=== Key Points to Observe ===")
            print("1. ğŸ” Check if object 3D coordinates are accurate")
            print("2. ğŸ” Verify object to 2D image projection is correct")
            print("3. ğŸ” Observe different colors for different object types")
            print("4. ğŸ” Confirm object annotations work with gripper annotations")

            print("\nğŸ’¡ Tip: Check the generated comparison images to analyze annotation effects")
        else:
            print("\nâŒ Object annotation test failed, please check environment configuration")

    else:  # choice == "3" æˆ–å…¶ä»–æƒ…å†µ
        print("\n=== æ‰§è¡Œå®‰å…¨æ¨¡å¼æµ‹è¯• ===")
        # è¿è¡Œå®‰å…¨æ¨¡å¼æµ‹è¯•
        results = safe_test()

        if results is not None:
            print("\nâœ… å®‰å…¨æ¨¡å¼æµ‹è¯•å®Œæˆï¼")
            print("\n=== æµ‹è¯•æ€»ç»“ ===")
            print(f"âœ“ å…±æµ‹è¯•äº† {len(results)} ä¸ªä½ç½®")
            print("âœ“ ä½¿ç”¨äº†æå°çš„åŠ¨ä½œå€¼é¿å…æ®µé”™è¯¯")
            print("âœ“ ç”Ÿæˆäº†æ ‡æ³¨å›¾åƒå’Œå¯¹æ¯”å›¾")

            print("\n=== è§‚å¯Ÿé‡ç‚¹ ===")
            print("1. ğŸ” éªŒè¯æ ‡æ³¨ç³»ç»Ÿçš„åŸºæœ¬åŠŸèƒ½")
            print("2. ğŸ” æ£€æŸ¥å¾®å°ä½ç§»æ˜¯å¦èƒ½è¢«æ­£ç¡®æ ‡æ³¨")
            print("3. ğŸ” ç¡®è®¤3Dåˆ°2DæŠ•å½±çš„å‡†ç¡®æ€§")
            print("4. ğŸ” éªŒè¯åæ ‡ç³»æ ‡æ³¨çš„æ­£ç¡®æ€§")

            print("\nğŸ’¡ å»ºè®®: å¦‚æœå®‰å…¨æ¨¡å¼æˆåŠŸï¼Œå¯ä»¥å°è¯•æ¨¡å¼2çš„å®Œæ•´æµ‹è¯•")
        else:
            print("\nâŒ å®‰å…¨æ¨¡å¼æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒé…ç½®")

def safe_test():
    """å®‰å…¨æµ‹è¯•å‡½æ•° - ä½¿ç”¨éå¸¸å°çš„åŠ¨ä½œå€¼"""
    print("=== å®‰å…¨æ¨¡å¼æµ‹è¯•ï¼ˆå°åŠ¨ä½œå€¼ï¼‰ ===\n")

    task_name = "widowx_carrot_on_plate"
    env = simpler_env.make(task_name)

    try:
        # åˆå§‹çŠ¶æ€
        obs, reset_info = env.reset(seed=42)
        initial_data = get_robot_pose_and_image_from_env(env, obs)

        print("ç¯å¢ƒä¿¡æ¯:")
        print(f"   åŠ¨ä½œç©ºé—´: {env.action_space}")
        print(f"   åŠ¨ä½œç©ºé—´ç»´åº¦: {env.action_space.shape}")
        print(f"   åŠ¨ä½œèŒƒå›´: {env.action_space.low} åˆ° {env.action_space.high}")

        print("\n1. åˆå§‹ä½ç½®:")
        print(f"   3Dä½ç½®: {initial_data['position']}")

        # éå¸¸ä¿å®ˆçš„åŠ¨ä½œ
        safe_actions = [
            {
                'name': 'åˆå§‹ä½ç½®',
                'action': None,
                'description': 'ç¯å¢ƒé‡ç½®åçš„åˆå§‹ä½ç½®'
            },
            {
                'name': 'å¾®å°ä¸Šç§»',
                'action': np.array([0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0], dtype=np.float32),
                'description': 'å¤¹çˆªå¾®å°å‘ä¸Šç§»åŠ¨1cm'
            },
            {
                'name': 'å¾®å°å³ç§»',
                'action': np.array([0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0], dtype=np.float32),
                'description': 'å¤¹çˆªå¾®å°å‘å³ç§»åŠ¨1cm'
            }
        ]

        annotated_images = []

        # å¤„ç†åˆå§‹ä½ç½®
        initial_annotated = create_annotated_image(initial_data, safe_actions[0]['name'],
                                                   'safe_gripper_pos_0_initial.png')
        annotated_images.append(initial_annotated)
        print(f"   ä¿å­˜: safe_gripper_pos_0_initial.png")

        # æ‰§è¡Œå®‰å…¨åŠ¨ä½œ
        for i, action_info in enumerate(safe_actions[1:], 1):
            print(f"\n{i+1}. {action_info['description']}:")

            try:
                action = action_info['action']
                print(f"   åŠ¨ä½œå‘é‡: {action}")

                # æ‰§è¡ŒåŠ¨ä½œ
                obs, reward, terminated, truncated, info = env.step(action)
                print("   âœ“ æ­¥è¿›æˆåŠŸ")

                # è·å–æ–°æ•°æ®
                new_data = get_robot_pose_and_image_from_env(env, obs)
                print(f"   æ–°ä½ç½®: {new_data['position']}")
                print(f"   ä½ç§»: {new_data['position'] - initial_data['position']}")

                # åˆ›å»ºå›¾åƒ
                filename = f'safe_gripper_pos_{i}_{action_info["name"].replace(" ", "_")}.png'
                annotated_img = create_annotated_image(new_data, action_info['name'], filename)
                annotated_images.append(annotated_img)
                print(f"   âœ“ ä¿å­˜: {filename}")

            except Exception as e:
                print(f"   âŒ å¤±è´¥: {e}")
                break

        # åˆ›å»ºç®€å•å¯¹æ¯”å›¾
        if len(annotated_images) >= 2:
            comparison = np.hstack(annotated_images[:3])  # æœ€å¤š3å¼ 
            cv2.imwrite('safe_comparison.png', cv2.cvtColor(comparison, cv2.COLOR_RGB2BGR))
            print(f"\nâœ“ ä¿å­˜å¯¹æ¯”å›¾: safe_comparison.png")

        print(f"\nâœ… å®‰å…¨æµ‹è¯•å®Œæˆï¼Œç”Ÿæˆ {len(annotated_images)} å¼ å›¾åƒ")
        return annotated_images

    except Exception as e:
        print(f"âŒ å®‰å…¨æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

    finally:
        env.close()

def draw_affordance_arrow(image, origin_2d, position_3d, quaternion,
                         intrinsic_matrix, extrinsic_matrix,
                         arrow_length=0.08, arrow_color=(0, 255, 0),
                         arrow_thickness=3, show_point=True):
    """
    åœ¨å›¾åƒä¸Šç»˜åˆ¶å¤¹çˆªæœå‘çš„affordanceç®­å¤´

    Args:
        image: è¾“å…¥å›¾åƒ
        origin_2d: å¤¹çˆªåœ¨å›¾åƒä¸­çš„2Dåæ ‡
        position_3d: å¤¹çˆªçš„3Dä¸–ç•Œåæ ‡
        quaternion: å§¿æ€å››å…ƒæ•° [w, x, y, z]
        intrinsic_matrix: ç›¸æœºå†…å‚çŸ©é˜µ
        extrinsic_matrix: ç›¸æœºå¤–å‚çŸ©é˜µ
        arrow_length: ç®­å¤´é•¿åº¦ (ç±³)
        arrow_color: ç®­å¤´é¢œè‰² (B, G, R)
        arrow_thickness: ç®­å¤´ç²—ç»†
        show_point: æ˜¯å¦æ˜¾ç¤ºå¤¹çˆªä½ç½®ç‚¹

    Returns:
        æ˜¯å¦ç»˜åˆ¶æˆåŠŸ
    """
    try:
        # å°†å››å…ƒæ•°è½¬æ¢ä¸ºæ—‹è½¬çŸ©é˜µ
        quat_scipy = [quaternion[1], quaternion[2], quaternion[3], quaternion[0]]
        rotation = Rotation.from_quat(quat_scipy)
        rotation_matrix = rotation.as_matrix()

        # Xè½´æ–¹å‘ä»£è¡¨å¤¹çˆªæœå‘ï¼ˆå‘å‰ï¼‰
        direction_3d = rotation_matrix @ np.array([arrow_length, 0, 0])

        # è®¡ç®—ç®­å¤´ç«¯ç‚¹çš„3Dä¸–ç•Œåæ ‡
        arrow_end_3d = position_3d + direction_3d

        # æŠ•å½±ç®­å¤´ç«¯ç‚¹åˆ°2D
        end_2d = project_3d_to_2d(arrow_end_3d, intrinsic_matrix, extrinsic_matrix)

        if end_2d is not None:
            u_orig, v_orig = origin_2d
            # ç»˜åˆ¶ç®­å¤´
            cv2.arrowedLine(image, (u_orig, v_orig), tuple(end_2d),
                          arrow_color, arrow_thickness, tipLength=0.3)

            # å¯é€‰ï¼šç»˜åˆ¶å¤¹çˆªä½ç½®ç‚¹
            if show_point:
                cv2.circle(image, (u_orig, v_orig), 5, arrow_color, -1)
                cv2.circle(image, (u_orig, v_orig), 7, (255, 255, 255), 2)

            return True
        else:
            return False

    except Exception as e:
        print(f"ç»˜åˆ¶affordanceç®­å¤´å¤±è´¥: {e}")
        return False

def draw_affordance_arrow_simple(image, origin_2d, quaternion,
                                 arrow_length=60, arrow_color=(0, 255, 0),
                                 arrow_thickness=3, show_point=True):
    """
    ç®€åŒ–ç‰ˆaffordanceç®­å¤´ç»˜åˆ¶ï¼ˆå½“æ— æ³•è¿›è¡Œ3DæŠ•å½±æ—¶ä½¿ç”¨ï¼‰
    ç›´æ¥åœ¨2Då›¾åƒå¹³é¢ä¸Šæ ¹æ®å››å…ƒæ•°ä¼°ç®—æ–¹å‘

    Args:
        image: è¾“å…¥å›¾åƒ
        origin_2d: å¤¹çˆªåœ¨å›¾åƒä¸­çš„2Dåæ ‡
        quaternion: å§¿æ€å››å…ƒæ•° [w, x, y, z]
        arrow_length: ç®­å¤´é•¿åº¦ï¼ˆåƒç´ ï¼‰
        arrow_color: ç®­å¤´é¢œè‰² (B, G, R)
        arrow_thickness: ç®­å¤´ç²—ç»†
        show_point: æ˜¯å¦æ˜¾ç¤ºå¤¹çˆªä½ç½®ç‚¹
    """
    u, v = origin_2d

    # å°†å››å…ƒæ•°è½¬æ¢ä¸ºæ—‹è½¬çŸ©é˜µ
    quat_scipy = [quaternion[1], quaternion[2], quaternion[3], quaternion[0]]
    rotation = Rotation.from_quat(quat_scipy)
    rotation_matrix = rotation.as_matrix()

    # è·å–Xè½´æ–¹å‘ï¼ˆå¤¹çˆªæœå‘ï¼‰
    direction_3d = rotation_matrix @ np.array([1, 0, 0])

    # ç®€åŒ–æŠ•å½±ï¼šå‡è®¾ä»ç¬¬ä¸‰äººç§°è§†è§’çœ‹
    # X -> å›¾åƒå³æ–¹å‘, Y -> å›¾åƒå·¦æ–¹å‘, Z -> å›¾åƒä¸Šæ–¹å‘
    dx = direction_3d[0] * arrow_length * 0.8 - direction_3d[1] * arrow_length * 0.3
    dy = -direction_3d[2] * arrow_length * 0.3 + direction_3d[0] * arrow_length * 0.2

    end_x = int(u + dx)
    end_y = int(v + dy)

    # ç»˜åˆ¶ç®­å¤´
    cv2.arrowedLine(image, (u, v), (end_x, end_y),
                   arrow_color, arrow_thickness, tipLength=0.3)

    # å¯é€‰ï¼šç»˜åˆ¶å¤¹çˆªä½ç½®ç‚¹
    if show_point:
        cv2.circle(image, (u, v), 5, arrow_color, -1)
        cv2.circle(image, (u, v), 7, (255, 255, 255), 2)

def add_affordance_to_observation(obs, env, arrow_length=0.08,
                                 arrow_color=(0, 255, 0),
                                 arrow_thickness=3, show_point=True):
    """
    å‘è§‚æµ‹å›¾åƒä¸­æ·»åŠ affordanceä¿¡æ¯ï¼ˆå¤¹çˆªæœå‘ç®­å¤´ï¼‰

    Args:
        obs: ç¯å¢ƒè§‚æµ‹å­—å…¸
        env: ç¯å¢ƒå®ä¾‹
        arrow_length: ç®­å¤´é•¿åº¦ï¼ˆç±³ï¼Œç”¨äº3DæŠ•å½±ï¼‰
        arrow_color: ç®­å¤´é¢œè‰² (B, G, R)ï¼Œé»˜è®¤ç»¿è‰²
        arrow_thickness: ç®­å¤´ç²—ç»†
        show_point: æ˜¯å¦æ˜¾ç¤ºå¤¹çˆªä½ç½®ç‚¹

    Returns:
        æ·»åŠ äº†affordanceçš„è§‚æµ‹å­—å…¸ï¼ˆæ·±æ‹·è´ï¼‰
    """
    import copy

    # æ·±æ‹·è´è§‚æµ‹ä»¥é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
    obs_with_affordance = copy.deepcopy(obs)

    try:
        # è·å–ä½å§¿ä¿¡æ¯
        data = get_robot_pose_and_image_from_env(env, obs)

        # è·å–æ‰€æœ‰ç›¸æœºçš„å›¾åƒ
        if "image" in obs_with_affordance:
            cam_imgs = obs_with_affordance["image"]

            for camera_name in cam_imgs.keys():
                if "rgb" in cam_imgs[camera_name]:
                    image = cam_imgs[camera_name]["rgb"]

                    # å°è¯•3Dåˆ°2DæŠ•å½±
                    position_2d = None
                    use_real_projection = False

                    camera_params = obs.get("camera_param", None)
                    if camera_params and camera_name in camera_params:
                        try:
                            params = camera_params[camera_name]
                            intrinsic = params['intrinsic_cv']
                            extrinsic = params['extrinsic_cv']

                            position_2d = project_3d_to_2d(
                                data['position'], intrinsic, extrinsic
                            )

                            if position_2d:
                                use_real_projection = True
                        except Exception:
                            pass

                    # å¦‚æœæŠ•å½±å¤±è´¥ï¼Œä½¿ç”¨å›¾åƒä¸­å¿ƒ
                    if position_2d is None:
                        h, w = image.shape[:2]
                        position_2d = [w // 2, h // 2]

                    # ç»˜åˆ¶affordanceç®­å¤´
                    if use_real_projection:
                        draw_affordance_arrow(
                            image, position_2d, data['position'],
                            data['quaternion'], intrinsic, extrinsic,
                            arrow_length, arrow_color, arrow_thickness, show_point
                        )
                    else:
                        draw_affordance_arrow_simple(
                            image, position_2d, data['quaternion'],
                            60, arrow_color, arrow_thickness, show_point
                        )

        return obs_with_affordance

    except Exception as e:
        print(f"æ·»åŠ affordanceå¤±è´¥: {e}")
        return obs_with_affordance

def test_affordance_visualization():
    """æµ‹è¯•affordanceå¯è§†åŒ–æ•ˆæœ"""
    print("=== æµ‹è¯•Affordanceå¯è§†åŒ– ===\n")

    task_name = "widowx_carrot_on_plate"
    env = simpler_env.make(task_name)

    try:
        obs, reset_info = env.reset(seed=42)

        print("æµ‹è¯•ä¸åŒçš„å¯è§†åŒ–é€‰é¡¹:")

        # æµ‹è¯•é…ç½®
        test_configs = [
            {
                'name': 'åŸå§‹å›¾åƒ',
                'add_affordance': False,
                'filename': 'test_original.png'
            },
            {
                'name': 'Affordance-ç»¿è‰²ç®­å¤´',
                'add_affordance': True,
                'arrow_color': (0, 255, 0),  # ç»¿è‰²
                'arrow_thickness': 3,
                'show_point': True,
                'filename': 'test_affordance_green.png'
            },
            {
                'name': 'Affordance-çº¢è‰²ç²—ç®­å¤´',
                'add_affordance': True,
                'arrow_color': (0, 0, 255),  # çº¢è‰²
                'arrow_thickness': 5,
                'show_point': True,
                'filename': 'test_affordance_red_thick.png'
            },
            {
                'name': 'Affordance-è“è‰²ç»†ç®­å¤´ï¼ˆæ— ç‚¹ï¼‰',
                'add_affordance': True,
                'arrow_color': (255, 0, 0),  # è“è‰²
                'arrow_thickness': 2,
                'show_point': False,
                'filename': 'test_affordance_blue_thin.png'
            }
        ]

        saved_images = []

        for i, config in enumerate(test_configs):
            print(f"\n{i+1}. {config['name']}:")

            if config['add_affordance']:
                # æ·»åŠ affordance
                obs_with_aff = add_affordance_to_observation(
                    obs, env,
                    arrow_color=config['arrow_color'],
                    arrow_thickness=config['arrow_thickness'],
                    show_point=config['show_point']
                )

                # è·å–å›¾åƒ
                if "image" in obs_with_aff:
                    cam_imgs = obs_with_aff["image"]
                    if "3rd_view_camera" in cam_imgs and "rgb" in cam_imgs["3rd_view_camera"]:
                        image = cam_imgs["3rd_view_camera"]["rgb"]
                    elif "base_camera" in cam_imgs and "rgb" in cam_imgs["base_camera"]:
                        image = cam_imgs["base_camera"]["rgb"]
                    else:
                        image = get_image_from_maniskill2_obs_dict(env, obs_with_aff)
                else:
                    image = get_image_from_maniskill2_obs_dict(env, obs_with_aff)
            else:
                # åŸå§‹å›¾åƒ
                if "image" in obs:
                    cam_imgs = obs["image"]
                    if "3rd_view_camera" in cam_imgs and "rgb" in cam_imgs["3rd_view_camera"]:
                        image = cam_imgs["3rd_view_camera"]["rgb"]
                    elif "base_camera" in cam_imgs and "rgb" in cam_imgs["base_camera"]:
                        image = cam_imgs["base_camera"]["rgb"]
                    else:
                        image = get_image_from_maniskill2_obs_dict(env, obs)
                else:
                    image = get_image_from_maniskill2_obs_dict(env, obs)

            # ä¿å­˜å›¾åƒ
            cv2.imwrite(config['filename'], cv2.cvtColor(image, cv2.COLOR_RGB2BGR))
            print(f"   ä¿å­˜: {config['filename']}")
            saved_images.append(image)

        # åˆ›å»ºå¯¹æ¯”å›¾
        print(f"\nåˆ›å»ºå¯¹æ¯”å›¾...")
        if len(saved_images) >= 2:
            # è°ƒæ•´å¤§å°
            target_h = 300
            resized = []
            for img in saved_images[:4]:  # æœ€å¤š4å¼ 
                h, w = img.shape[:2]
                target_w = int(w * target_h / h)
                resized.append(cv2.resize(img, (target_w, target_h)))

            # æ°´å¹³æ‹¼æ¥
            comparison = np.hstack(resized)
            cv2.imwrite('affordance_comparison.png', cv2.cvtColor(comparison, cv2.COLOR_RGB2BGR))
            print("ä¿å­˜å¯¹æ¯”å›¾: affordance_comparison.png")

        print("\nâœ… Affordanceå¯è§†åŒ–æµ‹è¯•å®Œæˆ!")
        print("\nç”Ÿæˆçš„æ–‡ä»¶:")
        for config in test_configs:
            print(f"  - {config['filename']}: {config['name']}")
        print("  - affordance_comparison.png: å¯¹æ¯”å›¾")

        return saved_images

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

    finally:
        env.close()

def test_affordance_with_actions():
    """æµ‹è¯•ä¸åŒåŠ¨ä½œä¸‹çš„affordanceå¯è§†åŒ–"""
    print("=== æµ‹è¯•ä¸åŒåŠ¨ä½œä¸‹çš„Affordance ===\n")

    task_name = "widowx_carrot_on_plate"
    env = simpler_env.make(task_name)

    try:
        obs, reset_info = env.reset(seed=42)

        # å®šä¹‰æµ‹è¯•åŠ¨ä½œ
        test_actions = [
            {
                'name': 'åˆå§‹ä½ç½®',
                'action': None
            },
            {
                'name': 'å‘ä¸Šç§»åŠ¨',
                'action': np.array([0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0], dtype=np.float32)
            },
            {
                'name': 'å‘å‰ç§»åŠ¨',
                'action': np.array([0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], dtype=np.float32)
            },
            {
                'name': 'æ—‹è½¬',
                'action': np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.15, 0.0], dtype=np.float32)
            }
        ]

        images_original = []
        images_with_affordance = []

        for i, action_info in enumerate(test_actions):
            print(f"\n{i+1}. {action_info['name']}:")

            # æ‰§è¡ŒåŠ¨ä½œï¼ˆå¦‚æœä¸æ˜¯åˆå§‹ä½ç½®ï¼‰
            if action_info['action'] is not None:
                try:
                    obs, _, _, _, _ = env.step(action_info['action'])
                except Exception as e:
                    print(f"   âš  åŠ¨ä½œæ‰§è¡Œå¤±è´¥: {e}")
                    continue

            # è·å–åŸå§‹å›¾åƒ
            if "image" in obs:
                cam_imgs = obs["image"]
                if "3rd_view_camera" in cam_imgs and "rgb" in cam_imgs["3rd_view_camera"]:
                    img_orig = cam_imgs["3rd_view_camera"]["rgb"].copy()
                elif "base_camera" in cam_imgs and "rgb" in cam_imgs["base_camera"]:
                    img_orig = cam_imgs["base_camera"]["rgb"].copy()
                else:
                    img_orig = get_image_from_maniskill2_obs_dict(env, obs)
            else:
                img_orig = get_image_from_maniskill2_obs_dict(env, obs)

            # æ·»åŠ affordance
            obs_with_aff = add_affordance_to_observation(obs, env)

            if "image" in obs_with_aff:
                cam_imgs = obs_with_aff["image"]
                if "3rd_view_camera" in cam_imgs and "rgb" in cam_imgs["3rd_view_camera"]:
                    img_aff = cam_imgs["3rd_view_camera"]["rgb"]
                elif "base_camera" in cam_imgs and "rgb" in cam_imgs["base_camera"]:
                    img_aff = cam_imgs["base_camera"]["rgb"]
                else:
                    img_aff = get_image_from_maniskill2_obs_dict(env, obs_with_aff)
            else:
                img_aff = get_image_from_maniskill2_obs_dict(env, obs_with_aff)

            # ä¿å­˜å›¾åƒ
            cv2.imwrite(f'action_{i}_original.png', cv2.cvtColor(img_orig, cv2.COLOR_RGB2BGR))
            cv2.imwrite(f'action_{i}_affordance.png', cv2.cvtColor(img_aff, cv2.COLOR_RGB2BGR))
            print(f"   ä¿å­˜: action_{i}_original.png, action_{i}_affordance.png")

            images_original.append(img_orig)
            images_with_affordance.append(img_aff)

        # åˆ›å»ºç½‘æ ¼å¯¹æ¯”å›¾
        print(f"\nåˆ›å»ºå¯¹æ¯”å›¾...")
        if len(images_original) >= 2:
            # ä¸Šä¸‹å¯¹æ¯”ï¼ˆåŸå§‹vs affordanceï¼‰
            target_h = 200

            row1_images = []  # åŸå§‹å›¾åƒè¡Œ
            row2_images = []  # affordanceå›¾åƒè¡Œ

            for img_orig, img_aff in zip(images_original, images_with_affordance):
                h, w = img_orig.shape[:2]
                target_w = int(w * target_h / h)
                row1_images.append(cv2.resize(img_orig, (target_w, target_h)))
                row2_images.append(cv2.resize(img_aff, (target_w, target_h)))

            row1 = np.hstack(row1_images)
            row2 = np.hstack(row2_images)
            grid = np.vstack([row1, row2])

            cv2.imwrite('affordance_actions_comparison.png', cv2.cvtColor(grid, cv2.COLOR_RGB2BGR))
            print("ä¿å­˜å¯¹æ¯”å›¾: affordance_actions_comparison.png")
            print("  ä¸Šè¡Œ: åŸå§‹å›¾åƒ")
            print("  ä¸‹è¡Œ: æ·»åŠ affordanceçš„å›¾åƒ")

        print("\nâœ… åŠ¨ä½œæµ‹è¯•å®Œæˆ!")
        return images_with_affordance

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

    finally:
        env.close()

def test_simple_object_annotation():
    """ç®€åŒ–çš„ç‰©ä½“æ ‡æ³¨æµ‹è¯•ï¼ˆä»¿ç…§affordanceé£æ ¼ï¼‰"""
    print("=== Simple Object Annotation Test ===\n")

    task_name = "widowx_carrot_on_plate"
    env = simpler_env.make(task_name)

    try:
        obs, reset_info = env.reset(seed=42)

        print("Testing different annotation options:")

        # æµ‹è¯•é…ç½® - ä»¿ç…§affordanceæµ‹è¯•çš„é£æ ¼
        test_configs = [
            {
                'name': 'Original Image',
                'include_objects': False,
                'filename': 'test_original_image.png'
            },
            {
                'name': 'Gripper Only',
                'include_objects': False,
                'filename': 'test_gripper_only.png'
            },
            {
                'name': 'Gripper + Objects',
                'include_objects': True,
                'filename': 'test_gripper_objects.png'
            }
        ]

        saved_images = []

        # è·å–æ•°æ®ï¼ˆä¼ é€’reset_infoä»¥è·å–ç‰©ä½“ä¿¡æ¯ï¼‰
        data = get_robot_pose_and_image_from_env(env, obs)

        # æ‰‹åŠ¨æ·»åŠ ç‰©ä½“ä¿¡æ¯ï¼ˆå› ä¸ºget_robot_pose_and_image_from_envæ²¡æœ‰reset_infoå‚æ•°ï¼‰
        objects_info = get_object_coordinates_from_env(env, obs, reset_info)
        data['objects_info'] = objects_info

        # æ˜¾ç¤ºæ‰¾åˆ°çš„ç‰©ä½“ä¿¡æ¯
        objects_info = data['objects_info']
        if objects_info:
            print(f"\nFound {len(objects_info)} objects:")
            for obj_key, obj_info in objects_info.items():
                name = obj_info['name']
                pos = obj_info['position']
                obj_type = obj_info['type']
                print(f"  - {name} ({obj_type}): [{pos[0]:.3f}, {pos[1]:.3f}, {pos[2]:.3f}]")
        else:
            print("\nNo objects found")

        for i, config in enumerate(test_configs):
            print(f"\n{i+1}. {config['name']}:")

            if config['name'] == 'Original Image':
                # åŸå§‹å›¾åƒ
                image = data['image']
            else:
                # åˆ›å»ºæ ‡æ³¨å›¾åƒ
                image = create_annotated_image(
                    data,
                    config['name'],
                    config['filename'],
                    include_objects=config['include_objects']
                )

            # ä¿å­˜å›¾åƒ
            if config['name'] == 'Original Image':
                cv2.imwrite(config['filename'], cv2.cvtColor(image, cv2.COLOR_RGB2BGR))

            print(f"   Saved: {config['filename']}")
            saved_images.append(image)

        # åˆ›å»ºå¯¹æ¯”å›¾
        print(f"\nCreating comparison image...")
        if len(saved_images) >= 2:
            # è°ƒæ•´å¤§å°
            target_h = 300
            resized = []
            for img in saved_images:
                h, w = img.shape[:2]
                target_w = int(w * target_h / h)
                resized.append(cv2.resize(img, (target_w, target_h)))

            # æ°´å¹³æ‹¼æ¥
            comparison = np.hstack(resized)
            cv2.imwrite('simple_object_comparison.png', cv2.cvtColor(comparison, cv2.COLOR_RGB2BGR))
            print("Saved comparison: simple_object_comparison.png")

        print("\nâœ… Simple object annotation test completed!")
        print("\nGenerated files:")
        for config in test_configs:
            print(f"  - {config['filename']}: {config['name']}")
        print("  - simple_object_comparison.png: Comparison image")

        return saved_images

    except Exception as e:
        print(f"âŒ Test failed: {e}")
        import traceback
        traceback.print_exc()
        return None

    finally:
        env.close()

def draw_out_of_bounds_indicator(image, position_2d, obj_info, img_w, img_h):
    """ä¸ºè¶…å‡ºå›¾åƒèŒƒå›´çš„ç‰©ä½“ç»˜åˆ¶è¾¹ç•ŒæŒ‡ç¤ºå™¨"""
    u, v = position_2d
    name = obj_info.get('name', 'unknown')
    color = obj_info.get('color', (0, 255, 0))

    # å°†è¶…å‡ºèŒƒå›´çš„åæ ‡é™åˆ¶åˆ°è¾¹ç•Œ
    u_clamped = max(10, min(img_w - 10, u))
    v_clamped = max(10, min(img_h - 10, v))

    # ç»˜åˆ¶è¾¹ç•ŒæŒ‡ç¤ºå™¨
    cv2.circle(image, (u_clamped, v_clamped), 6, color, 2)  # ç©ºå¿ƒåœ†è¡¨ç¤ºè¶…å‡ºèŒƒå›´
    cv2.circle(image, (u_clamped, v_clamped), 8, (255, 255, 255), 1)

    # æ·»åŠ ç®­å¤´æŒ‡ç¤ºå®é™…æ–¹å‘
    if u < 0:
        # åœ¨å·¦è¾¹ç•Œï¼Œç®­å¤´æŒ‡å‘å·¦
        cv2.arrowedLine(image, (u_clamped + 5, v_clamped), (u_clamped - 5, v_clamped), color, 2)
    elif u >= img_w:
        # åœ¨å³è¾¹ç•Œï¼Œç®­å¤´æŒ‡å‘å³
        cv2.arrowedLine(image, (u_clamped - 5, v_clamped), (u_clamped + 5, v_clamped), color, 2)

    if v < 0:
        # åœ¨ä¸Šè¾¹ç•Œï¼Œç®­å¤´æŒ‡å‘ä¸Š
        cv2.arrowedLine(image, (u_clamped, v_clamped + 5), (u_clamped, v_clamped - 5), color, 2)
    elif v >= img_h:
        # åœ¨ä¸‹è¾¹ç•Œï¼Œç®­å¤´æŒ‡å‘ä¸‹
        cv2.arrowedLine(image, (u_clamped, v_clamped - 5), (u_clamped, v_clamped + 5), color, 2)

    # æ·»åŠ æ ‡ç­¾
    label = f"{name[:8]}*"  # åŠ *è¡¨ç¤ºè¶…å‡ºèŒƒå›´
    cv2.putText(image, label, (u_clamped + 12, v_clamped - 5),
               cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)

    # æ·»åŠ åæ ‡ä¿¡æ¯
    coord_text = f"({u},{v})"
    cv2.putText(image, coord_text, (u_clamped + 12, v_clamped + 10),
               cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1)

def draw_failed_projection_info(image, obj_info, total_objects):
    """ä¸ºæŠ•å½±å¤±è´¥çš„ç‰©ä½“æ˜¾ç¤ºä¿¡æ¯"""
    name = obj_info.get('name', 'unknown')
    color = obj_info.get('color', (0, 255, 0))

    # åœ¨å›¾åƒå³ä¸Šè§’æ˜¾ç¤ºå¤±è´¥ä¿¡æ¯
    h, w = image.shape[:2]
    y_pos = 50 + (total_objects * 15)  # é¿å…é‡å 

    fail_text = f"{name[:12]}: PROJ FAIL"
    cv2.putText(image, fail_text, (w - 200, y_pos),
               cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 255), 1)

def create_enhanced_annotated_image(data, step_name, save_name=None, include_objects=True):
    """åˆ›å»ºå¢å¼ºçš„æ ‡æ³¨å›¾åƒï¼Œæ˜¾ç¤ºæ‰€æœ‰ç‰©ä½“ï¼ˆåŒ…æ‹¬è¶…å‡ºèŒƒå›´çš„ï¼‰"""
    image = data['image'].copy()
    h, w = image.shape[:2]

    # è·å–ç›¸æœºå‚æ•°
    intrinsic = None
    extrinsic = None
    use_real_projection = False

    if data['camera_params'] and data['camera_name'] in data['camera_params']:
        try:
            params = data['camera_params'][data['camera_name']]
            intrinsic = params['intrinsic_cv']
            extrinsic = params['extrinsic_cv']
            use_real_projection = True
        except Exception:
            pass

    # 1. å¤„ç†å¤¹çˆªä½å§¿æ ‡æ³¨
    gripper_position_2d = None
    if use_real_projection:
        gripper_position_2d = project_3d_to_2d(
            data['position'], intrinsic, extrinsic
        )

    if gripper_position_2d is None:
        gripper_position_2d = [w // 2, h // 2]

    if not (0 <= gripper_position_2d[0] < w and 0 <= gripper_position_2d[1] < h):
        gripper_position_2d = [w // 2, h // 2]

    # ç»˜åˆ¶æ ‡æ³¨
    annotated_image = image.copy()

    # ç»˜åˆ¶å¤¹çˆªæœå‘ç®­å¤´
    if use_real_projection:
        success = draw_affordance_arrow(
            annotated_image, gripper_position_2d, data['position'], data['quaternion'],
            intrinsic, extrinsic, arrow_length=0.08, arrow_color=(0, 255, 0),
            arrow_thickness=3, show_point=True
        )
        if not success:
            draw_affordance_arrow_simple(
                annotated_image, gripper_position_2d, data['quaternion'],
                arrow_length=60, arrow_color=(0, 255, 0), arrow_thickness=3, show_point=True
            )
    else:
        draw_affordance_arrow_simple(
            annotated_image, gripper_position_2d, data['quaternion'],
            arrow_length=60, arrow_color=(0, 255, 0), arrow_thickness=3, show_point=True
        )

    # ç»˜åˆ¶å¤¹çˆªä½å§¿ä¿¡æ¯
    annotated_image = draw_pose_annotation(
        annotated_image, gripper_position_2d,
        data['quaternion'], data['gripper_width']
    )

    # 2. å¤„ç†ç‰©ä½“æ ‡æ³¨ - å¢å¼ºç‰ˆæœ¬ï¼ˆæ˜¾ç¤ºæ‰€æœ‰ç‰©ä½“ï¼‰
    if include_objects and 'objects_info' in data:
        objects_info = data['objects_info']
        in_bounds_count = 0
        out_bounds_count = 0
        failed_count = 0

        for obj_key, obj_info in objects_info.items():
            obj_position_3d = obj_info['position']

            # æŠ•å½±ç‰©ä½“ä½ç½®åˆ°2D
            obj_position_2d = None
            if use_real_projection:
                obj_position_2d = project_3d_to_2d(
                    obj_position_3d, intrinsic, extrinsic
                )

            # å¤„ç†æŠ•å½±ç»“æœ
            if obj_position_2d is not None:
                u, v = obj_position_2d
                in_bounds = (0 <= u < w and 0 <= v < h)

                if in_bounds:
                    # åœ¨å›¾åƒèŒƒå›´å†…ï¼Œæ­£å¸¸ç»˜åˆ¶
                    draw_simple_object_marker(annotated_image, obj_position_2d, obj_info)
                    in_bounds_count += 1
                else:
                    # è¶…å‡ºèŒƒå›´ï¼Œç»˜åˆ¶è¾¹ç•ŒæŒ‡ç¤ºå™¨
                    draw_out_of_bounds_indicator(annotated_image, obj_position_2d, obj_info, w, h)
                    out_bounds_count += 1
            else:
                # æŠ•å½±å¤±è´¥ï¼Œåœ¨å›¾åƒè§’è½æ˜¾ç¤ºä¿¡æ¯
                draw_failed_projection_info(annotated_image, obj_info, failed_count)
                failed_count += 1

    # 3. æ·»åŠ æ­¥éª¤æ ‡ç­¾
    cv2.putText(annotated_image, f"{step_name}", (10, 30),
               cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
    cv2.putText(annotated_image, f"{step_name}", (11, 31),
               cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 3)

    # 4. æ·»åŠ è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
    if include_objects and 'objects_info' in data:
        total_count = len(data['objects_info'])
        stats_lines = [
            f"Objects: {total_count} total",
            f"In view: {in_bounds_count}",
            f"Out of bounds: {out_bounds_count}",
            f"Proj failed: {failed_count}"
        ]

        for i, line in enumerate(stats_lines):
            y_pos = h - 60 + i * 12
            cv2.putText(annotated_image, line, (10, y_pos),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
            cv2.putText(annotated_image, line, (11, y_pos + 1),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 0), 2)

    # 5. ä¿å­˜å›¾åƒ
    if save_name:
        cv2.imwrite(save_name, cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))

    return annotated_image

def quick_test():
    """å¿«é€Ÿæµ‹è¯•å‡½æ•° - ç›´æ¥è¿è¡Œå¤šä½ç½®æµ‹è¯•"""
    print("å¿«é€Ÿå¤šä½ç½®æµ‹è¯•æ¨¡å¼")
    print("=" * 40)
    results = test_multiple_gripper_positions()
    if results:
        print(f"\nâœ… æˆåŠŸç”Ÿæˆ {len(results)} ä¸ªä¸åŒä½ç½®çš„æ ‡æ³¨å›¾åƒï¼")
    return results

if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1:
        if sys.argv[1] == "--quick":
            quick_test()
        elif sys.argv[1] == "--safe":
            safe_test()
        elif sys.argv[1] == "--affordance":
            test_affordance_visualization()
        elif sys.argv[1] == "--affordance-actions":
            test_affordance_with_actions()
        elif sys.argv[1] == "--objects":
            test_simple_object_annotation()
        else:
            main()
    else:
        main()